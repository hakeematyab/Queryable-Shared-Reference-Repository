{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32e61725-f1b9-4c43-a50a-8c1e3612e8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/hakeem.at/Queryable-Shared-Reference-Repository/.venv/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-18 15:41:07 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "cache_dir ='/scratch/hakeem.at/Queryable-Shared-Reference-Repository/notebooks/pretrained_models'\n",
    "\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = cache_dir\n",
    "\n",
    "import json\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1326ce7a-a08b-4999-9420-2a5fca2ca3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total evaluation questions: 1500\n",
      "Question type distribution:\n",
      "  answerable: 500\n",
      "  borderline: 500\n",
      "  unanswerable: 500\n"
     ]
    }
   ],
   "source": [
    "input_file = \"prompt_thresholding_eval_dataset.json\"\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    eval_dataset = json.load(f)\n",
    "\n",
    "eval_data = []\n",
    "random.seed(42)\n",
    "random.shuffle(eval_dataset)\n",
    "max_data_points = 500\n",
    "for idx, item in enumerate(eval_dataset):\n",
    "    if not item: \n",
    "        continue\n",
    "    if idx==max_data_points:\n",
    "        break\n",
    "    \n",
    "    questions = [\n",
    "        {\n",
    "            'question': item['answerable_query'],\n",
    "            'question_type': 'answerable',\n",
    "            'ground_truth': item['excerpt'],\n",
    "            'chunk': item['chunk'],\n",
    "            'source': item['source']\n",
    "        },\n",
    "        {\n",
    "            'question': item['imaginative_query'],\n",
    "            'question_type': 'borderline',\n",
    "            'ground_truth': None,\n",
    "            'chunk': item['chunk'],\n",
    "            'source': item['source']\n",
    "        },\n",
    "        {\n",
    "            'question': item['unanswerable_query'],\n",
    "            'question_type': 'unanswerable',\n",
    "            'ground_truth': None,\n",
    "            'chunk': item['chunk'],\n",
    "            'source': item['source']\n",
    "        }\n",
    "    ]\n",
    "    eval_data.extend(questions)\n",
    "    \n",
    "\n",
    "print(f\"Total evaluation questions: {len(eval_data)}\")\n",
    "print(f\"Question type distribution:\")\n",
    "for qtype in ['answerable', 'borderline', 'unanswerable']:\n",
    "    count = sum(1 for q in eval_data if q['question_type'] == qtype)\n",
    "    print(f\"  {qtype}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24420f0-e2b2-4041-aa5d-53d4dcfd7419",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS = {\n",
    "    'baseline': \"\"\"\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "<answer>\"\"\",\n",
    "    \n",
    "    'explicit_idk': \"\"\"<instructions>\n",
    "Answer the question using ONLY information from the context provided below. If the context does not contain enough information to answer the question, respond with exactly: \"I don't know.\"\n",
    "</instructions>\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "<answer>\"\"\",\n",
    "    \n",
    "    'confidence_threshold': \"\"\"<instructions>\n",
    "Answer only if you are at least 80% confident that your answer is correct based on the context provided. \n",
    "\n",
    "Scoring system:\n",
    "- Mistakes cost you 4 points (calculated as 80/(100-80) = 4)\n",
    "- Correct answers earn 1 point\n",
    "- Saying \"I don't know\" earns 0 points\n",
    "\n",
    "If your confidence is below 80%, respond with exactly: \"I don't know.\"\n",
    "</instructions>\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "<answer>\"\"\",\n",
    "    \n",
    "    'confidence_rubric': \"\"\"<instructions>\n",
    "Before answering, evaluate these 5 criteria:\n",
    "\n",
    "1. The answer is explicitly stated in the context (not requiring inference or speculation)\n",
    "2. All necessary information to answer the question is present in the context\n",
    "3. There is no contradictory or ambiguous information in the context\n",
    "4. You can identify the specific sentence(s) in the context that support your answer\n",
    "5. The question asks for information of the type provided in the context (not future predictions, comparisons to unmentioned work, etc.)\n",
    "\n",
    "Only answer if at least 4 out of 5 criteria are clearly satisfied. Otherwise, respond with exactly: \"I don't know.\"\n",
    "</instructions>\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "<answer>\"\"\"\n",
    "}\n",
    "\n",
    "model_id = \"Qwen/Qwen3-8B\"  \n",
    "\n",
    "gpu_memory_utilization = 0.95\n",
    "max_model_len = 4096\n",
    "max_num_seqs = 64\n",
    "enforce_eager = True\n",
    "\n",
    "model = LLM(\n",
    "    model=model_id,\n",
    "    gpu_memory_utilization=gpu_memory_utilization,\n",
    "    max_model_len=max_model_len,\n",
    "    max_num_seqs=max_num_seqs,\n",
    "    enforce_eager=enforce_eager,\n",
    "    trust_remote_code=True,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e236201a-93bd-4e64-b808-30dcdbe23a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=0,\n",
    "    max_tokens=512,\n",
    "    stop = [\"</answer>\"],\n",
    "    # include_stop_str_in_output = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e1ddb36-459e-4dc7-987c-c82ea2ebddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_prompt = \"\"\"<context>\n",
    "# Researchers found trash at the bottom of the ocean.\n",
    "# </context>\n",
    "\n",
    "# <question>\n",
    "# Which brand of beer did the researchers found at the bottom of the ocean\n",
    "# </question>\n",
    "\n",
    "# <answer>\"\"\"\n",
    "\n",
    "# # test_params = SamplingParams(temperature=0, max_tokens=100)\n",
    "# output = model.generate([test_prompt], sampling_params)[0]\n",
    "# print(f\"âœ… Response: {output.outputs[0].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5de8399-de2a-40b1-978d-3840e762670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "output_file = \"prompt_thresholding_responses.jsonl\"\n",
    "batch_size = 2048\n",
    "\n",
    "for prompt_type, prompt_template in PROMPTS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running inference with prompt type: {prompt_type}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for idx in tqdm(range(0, len(eval_data), batch_size), desc=f\"{prompt_type}\"):\n",
    "        batch_data = eval_data[idx:min(idx + batch_size, len(eval_data))]\n",
    "        \n",
    "        batch_prompts = []\n",
    "        for item in batch_data:\n",
    "            prompt = prompt_template.format(\n",
    "                context=item['chunk'],\n",
    "                question=item['question']\n",
    "            )\n",
    "            batch_prompts.append(prompt)\n",
    "        \n",
    "        try:\n",
    "            responses = model.generate(\n",
    "                batch_prompts,\n",
    "                sampling_params,\n",
    "            )\n",
    "            \n",
    "            for item, response in zip(batch_data, responses):\n",
    "                result = {\n",
    "                    'question': item['question'],\n",
    "                    'question_type': item['question_type'],\n",
    "                    'context': item['chunk'],\n",
    "                    'source': item['source'],\n",
    "                    'ground_truth': item['ground_truth'],\n",
    "                    'prompt_type': prompt_type,\n",
    "                    'raw_response': response.outputs[0].text.strip(),\n",
    "                }\n",
    "                results.append(result)\n",
    "                \n",
    "                # with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                #     f.write(json.dumps(results, ensure_ascii=False) + \"\\n\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {idx}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a8b90ee-bcdc-4c39-b6c4-b9474eb4ef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(results, ensure_ascii=False) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-QSRR2.0",
   "language": "python",
   "name": "qsrr2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
