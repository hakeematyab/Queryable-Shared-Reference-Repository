{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e61725-f1b9-4c43-a50a-8c1e3612e8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cache_dir ='/scratch/hakeem.at/Queryable-Shared-Reference-Repository/notebooks/pretrained_models'\n",
    "\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = cache_dir\n",
    "\n",
    "import json\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1326ce7a-a08b-4999-9420-2a5fca2ca3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"context_expansion_dataset.jsonl\"\n",
    "eval_data = pd.read_json(input_file, lines=True)\n",
    "\n",
    "print(f\"Total evaluation samples: {len(eval_data)}\")\n",
    "print(f\"\\nDistribution by expansion type and context %:\")\n",
    "print(eval_data.groupby(['expansion_type', 'target_pct']).size().unstack())\n",
    "print(f\"\\nQuestion type distribution:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc914012-d554-4435-968d-5c6984fb0e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"<instructions>\n",
    "Answer the question using ONLY information from the context provided below. If the context does not contain enough information to answer the question, respond with exactly: \"I don't know.\"\n",
    "</instructions>\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "<answer>\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24420f0-e2b2-4041-aa5d-53d4dcfd7419",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen3-8B\"  \n",
    "\n",
    "gpu_memory_utilization = 0.95\n",
    "max_model_len = 32768  \n",
    "max_num_seqs = 32\n",
    "enforce_eager = True\n",
    "\n",
    "model = LLM(\n",
    "    model=model_id,\n",
    "    gpu_memory_utilization=gpu_memory_utilization,\n",
    "    max_model_len=max_model_len,\n",
    "    max_num_seqs=max_num_seqs,\n",
    "    enforce_eager=enforce_eager,\n",
    "    trust_remote_code=True,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e236201a-93bd-4e64-b808-30dcdbe23a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=0,\n",
    "    max_tokens=512,\n",
    "    stop = [\"</answer>\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1ddb36-459e-4dc7-987c-c82ea2ebddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_prompt = \"\"\"<context>\n",
    "# Researchers found trash at the bottom of the ocean.\n",
    "# </context>\n",
    "\n",
    "# <question>\n",
    "# Which brand of beer did the researchers found at the bottom of the ocean\n",
    "# </question>\n",
    "\n",
    "# <answer>\"\"\"\n",
    "\n",
    "# # test_params = SamplingParams(temperature=0, max_tokens=100)\n",
    "# output = model.generate([test_prompt], sampling_params)[0]\n",
    "# print(f\"âœ… Response: {output.outputs[0].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5de8399-de2a-40b1-978d-3840e762670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "output_file = \"context_expansion_responses.jsonl\"\n",
    "\n",
    "for target_pct in sorted(eval_data['target_pct'].unique()):\n",
    "    subset = eval_data[eval_data['target_pct'] == target_pct]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing context length: {int(target_pct*100)}% ({subset.iloc[0]['target_tokens']} tokens)\")\n",
    "    print(f\"Samples: {len(subset)}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    if target_pct <= 0.25:\n",
    "        batch_size = 512\n",
    "    elif target_pct <= 0.50:\n",
    "        batch_size = 256\n",
    "    elif target_pct <= 0.75:\n",
    "        batch_size = 128\n",
    "    else:\n",
    "        batch_size = 64\n",
    "    \n",
    "    for idx in tqdm(range(0, len(subset), batch_size), desc=f\"{int(target_pct*100)}%\"):\n",
    "        batch_df = subset.iloc[idx:min(idx + batch_size, len(subset))]\n",
    "        \n",
    "        batch_prompts = []\n",
    "        for _, row in batch_df.iterrows():\n",
    "            prompt = PROMPT_TEMPLATE.format(\n",
    "                context=row['expanded_context'],\n",
    "                question=row['query']\n",
    "            )\n",
    "            batch_prompts.append(prompt)\n",
    "        \n",
    "        try:\n",
    "            responses = model.generate(batch_prompts, sampling_params)\n",
    "            \n",
    "            for (_, row), response in zip(batch_df.iterrows(), responses):\n",
    "                result = {\n",
    "                    'original_idx': row['original_idx'],\n",
    "                    'source': row['source'],\n",
    "                    'question_type': row['question_type'],\n",
    "                    'query': row['query'],\n",
    "                    'original_context': row['original_context'],\n",
    "                    'expansion_type': row['expansion_type'],\n",
    "                    'target_pct': row['target_pct'],\n",
    "                    'target_tokens': row['target_tokens'],\n",
    "                    'actual_tokens': row['actual_tokens'],\n",
    "                    'ground_truth': row['ground_truth'],\n",
    "                    'raw_response': response.outputs[0].text.strip(),\n",
    "                }\n",
    "                results.append(result)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {idx} at {target_pct}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Checkpoint after each percentage\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in results:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"Checkpoint saved: {len(results)} results\")\n",
    "print(f\"\\nCompleted! Total results: {len(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8b90ee-bcdc-4c39-b6c4-b9474eb4ef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(json.dumps(results, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e457e4-7941-44ec-b9de-abac367db6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nIDK rate by context length:\")\n",
    "print(results_df.groupby('target_pct')['raw_response'].apply(\n",
    "    lambda x: (x.str.lower().str.contains(\"don't know\").sum() / len(x) * 100)\n",
    ").round(1).rename(\"IDK %\"))\n",
    "\n",
    "print(\"\\nIDK rate by expansion type:\")\n",
    "print(results_df.groupby('expansion_type')['raw_response'].apply(\n",
    "    lambda x: (x.str.lower().str.contains(\"don't know\").sum() / len(x) * 100)\n",
    ").round(1).rename(\"IDK %\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f8d154-45d2-4936-b867-e8184d469393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-QSRR2.0",
   "language": "python",
   "name": "qsrr2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
