{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89593edc-3e79-4d1c-aff2-06fe56c2b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97adf1d2-8071-4a2e-852d-1112a74a3960",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = '/courses/DS5500.202610/data/team1/raw_data/'\n",
    "raw_data_dir_names = ['ethan', 'sai']\n",
    "raw_data_dirs = [raw_data_path+raw_data_dir_name for raw_data_dir_name in raw_data_dir_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ada1d89c-879f-4b68-9192-4466f7b2433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "paths = {}\n",
    "for dir in raw_data_dirs:\n",
    "    for file in Path(dir).rglob(\"*\"):\n",
    "        if file.is_file():\n",
    "            doc_format = file.suffix.lower() if file.suffix else \"no_extension\"\n",
    "            counter[doc_format]+=1\n",
    "            list_of_paths = paths.get(doc_format,[])\n",
    "            list_of_paths.append(str(file))\n",
    "            paths[doc_format] = list_of_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "017cf743-5e32-4e54-94ad-49a5751963d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document Format</th>\n",
       "      <th>Document Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.ris</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.html</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.pdf</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no_extension</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.docx</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Document Format  Document Count\n",
       "0            .ris               3\n",
       "1           .html              43\n",
       "2            .pdf             253\n",
       "3    no_extension               2\n",
       "4           .docx               1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_summary_df = pd.DataFrame(counter.items(), columns=[\"Document Format\", \"Document Count\"])\n",
    "documents_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6df2989-5f15-42b7-8e9c-996740a1fa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pdfkit in ./.local/lib/python3.12/site-packages (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pdfkit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ba45b98-2893-401c-9ec6-e0893d2d004e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: PyMuPDF in ./.local/lib/python3.12/site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14b9f4c0-37e3-495c-a463-1ee04e156c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mammoth in ./.local/lib/python3.12/site-packages (1.11.0)\n",
      "Requirement already satisfied: cobble<0.2,>=0.1.3 in ./.local/lib/python3.12/site-packages (from mammoth) (0.1.4)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install mammoth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75ca9fff-01e7-4eeb-855a-93d228b1e870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: camelot-py[cv] in ./.local/lib/python3.12/site-packages (1.0.9)\n",
      "\u001b[33mWARNING: camelot-py 1.0.9 does not provide the extra 'cv'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: click>=8.0.1 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from camelot-py[cv]) (8.1.7)\n",
      "Requirement already satisfied: chardet>=5.1.0 in ./.local/lib/python3.12/site-packages (from camelot-py[cv]) (5.2.0)\n",
      "Requirement already satisfied: numpy>=1.26.1 in ./.local/lib/python3.12/site-packages (from camelot-py[cv]) (2.2.6)\n",
      "Requirement already satisfied: openpyxl>=3.1.0 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from camelot-py[cv]) (3.1.2)\n",
      "Requirement already satisfied: pdfminer-six>=20240706 in ./.local/lib/python3.12/site-packages (from camelot-py[cv]) (20250506)\n",
      "Requirement already satisfied: pypdf<6.0,>=4.0 in ./.local/lib/python3.12/site-packages (from camelot-py[cv]) (5.9.0)\n",
      "Requirement already satisfied: pandas>=2.2.2 in ./.local/lib/python3.12/site-packages (from camelot-py[cv]) (2.3.3)\n",
      "Requirement already satisfied: tabulate>=0.9.0 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from camelot-py[cv]) (0.9.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.7.0.68 in ./.local/lib/python3.12/site-packages (from camelot-py[cv]) (4.12.0.88)\n",
      "Requirement already satisfied: pypdfium2>=4 in ./.local/lib/python3.12/site-packages (from camelot-py[cv]) (4.30.0)\n",
      "Requirement already satisfied: pillow>=10.4.0 in ./.local/lib/python3.12/site-packages (from camelot-py[cv]) (11.3.0)\n",
      "Requirement already satisfied: et-xmlfile in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from openpyxl>=3.1.0->camelot-py[cv]) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from pandas>=2.2.2->camelot-py[cv]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from pandas>=2.2.2->camelot-py[cv]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from pandas>=2.2.2->camelot-py[cv]) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from pdfminer-six>=20240706->camelot-py[cv]) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from pdfminer-six>=20240706->camelot-py[cv]) (42.0.5)\n",
      "Requirement already satisfied: cffi>=1.12 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[cv]) (1.16.0)\n",
      "Requirement already satisfied: six>=1.5 in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=2.2.2->camelot-py[cv]) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[cv]) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install \"camelot-py[cv]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "241f29f5-2ae7-4c95-921a-0539f880df80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'.pdf': 253, '.html': 43, '.ris': 3, 'no_extension': 2, '.docx': 1})\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "raw_data_path = '/courses/DS5500.202610/data/team1/raw_data/'\n",
    "raw_data_dir_names = ['ethan', 'sai']\n",
    "raw_data_dirs = [raw_data_path + name for name in raw_data_dir_names]\n",
    "\n",
    "# Output directories in your home space\n",
    "processed_text_dir = Path('/home/anbarasan.p/processed_text')\n",
    "processed_text_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "processed_tables_dir = Path('/home/anbarasan.p/processed_tables')\n",
    "processed_tables_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "counter = Counter()\n",
    "paths = {}\n",
    "\n",
    "for dir in raw_data_dirs:\n",
    "    for file in Path(dir).rglob(\"*\"):\n",
    "        if file.is_file():\n",
    "            doc_format = file.suffix.lower() if file.suffix else \"no_extension\"\n",
    "            counter[doc_format] += 1\n",
    "            paths.setdefault(doc_format, []).append(str(file))\n",
    "\n",
    "print(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af1554be-9457-441e-a6c3-9c9d03d2111e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 299 total files to process (253 PDFs, 46 non-PDFs).\n",
      "\n",
      "MuPDF error: format error: object out of range (640 0 R); xref size 503\n",
      "\n",
      "\n",
      " Extraction complete — results summary:\n",
      " 256 succeeded, 43 failed/skipped\n",
      "Outputs saved under:\n",
      "- Text: /home/anbarasan.p/processed_text\n",
      "- Images: /home/anbarasan.p/processed_images\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "from bs4 import BeautifulSoup\n",
    "from docx import Document\n",
    "import fitz  # PyMuPDF\n",
    "import warnings\n",
    "import contextlib\n",
    "import traceback\n",
    "import os\n",
    "\n",
    "# config\n",
    "processed_base = Path('/home/anbarasan.p')\n",
    "processed_text_dir = processed_base / 'processed_text'\n",
    "processed_images_dir = processed_base / 'processed_images'\n",
    "\n",
    "for d in [processed_text_dir, processed_images_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Assuming `paths` is a dict like {'pdf': [...], '.html': [...], '.docx': [...], 'no_extension': [...]}\n",
    "pdf_files = paths.get('.pdf', [])\n",
    "non_pdf_files = paths.get('.html', []) + paths.get('.docx', []) + paths.get('no_extension', [])\n",
    "all_files = pdf_files + non_pdf_files\n",
    "print(f\"Found {len(all_files)} total files to process ({len(pdf_files)} PDFs, {len(non_pdf_files)} non-PDFs).\\n\")\n",
    "\n",
    "\n",
    "# extraction function\n",
    "def extract_text(file_path):\n",
    "    file_path = Path(file_path)\n",
    "    name = file_path.stem\n",
    "    suffix = file_path.suffix.lower()\n",
    "    text_out = processed_text_dir / f\"{name}.txt\"\n",
    "    image_out = processed_images_dir / f\"{name}_images.txt\"\n",
    "\n",
    "    result = {\"file\": name, \"status\": \"success\", \"type\": suffix or \"no_extension\"}\n",
    "\n",
    "    # Skip unreadable or missing files \n",
    "    if not file_path.exists() or not os.access(file_path, os.R_OK):\n",
    "        print(f\" Skipped {name}: File not readable or missing.\")\n",
    "        result[\"status\"] = \"error\"\n",
    "        result[\"error\"] = \"Permission denied or file not readable\"\n",
    "        return result\n",
    "\n",
    "    try:\n",
    "        text = \"\"\n",
    "\n",
    "        # pdf\n",
    "        if suffix == \".pdf\":\n",
    "            try:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    with contextlib.redirect_stderr(open(os.devnull, 'w')):\n",
    "                        with fitz.open(file_path) as doc:\n",
    "                            text = \"\".join([page.get_text(\"text\") for page in doc])\n",
    "            except Exception as e:\n",
    "                print(f\" Skipped {name}: PDF read failed ({e}).\")\n",
    "                result[\"status\"] = \"error\"\n",
    "                result[\"error\"] = str(e)\n",
    "                return result\n",
    "\n",
    "        # html\n",
    "        elif suffix == \".html\":\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                    soup = BeautifulSoup(f, \"html.parser\")\n",
    "                    text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "            except Exception as e:\n",
    "                print(f\" Skipped {name}: HTML parse failed ({e}).\")\n",
    "                result[\"status\"] = \"error\"\n",
    "                result[\"error\"] = str(e)\n",
    "                return result\n",
    "\n",
    "        # docx\n",
    "        elif suffix == \".docx\":\n",
    "            try:\n",
    "                doc = Document(file_path)\n",
    "                text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "            except Exception as e:\n",
    "                print(f\" Skipped {name}: DOCX read failed ({e}).\")\n",
    "                result[\"status\"] = \"error\"\n",
    "                result[\"error\"] = str(e)\n",
    "                return result\n",
    "\n",
    "        # no extension\n",
    "        elif suffix == \"\" or suffix == \"no_extension\":\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                    text = f.read()\n",
    "            except Exception as e:\n",
    "                print(f\" Skipped {name}: Text read failed ({e}).\")\n",
    "                result[\"status\"] = \"error\"\n",
    "                result[\"error\"] = str(e)\n",
    "                return result\n",
    "\n",
    "        else:\n",
    "            print(f\" Skipped {name}: Unsupported file type ({suffix}).\")\n",
    "            result[\"status\"] = \"error\"\n",
    "            result[\"error\"] = f\"Unsupported type: {suffix}\"\n",
    "            return result\n",
    "\n",
    "        # Write extracted text and placeholder \n",
    "        text_out.write_text(text.strip(), encoding=\"utf-8\")\n",
    "        image_out.write_text(\"[IMAGE PLACEHOLDER]\\n\", encoding=\"utf-8\")\n",
    "\n",
    "    except Exception as e:\n",
    "        result[\"status\"] = \"error\"\n",
    "        result[\"error\"] = str(e)\n",
    "        print(f\" Skipped {name}: Unexpected error ({e}).\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# parallel execution\n",
    "n_jobs = 2\n",
    "results = Parallel(n_jobs=n_jobs, backend=\"loky\")(\n",
    "    delayed(extract_text)(file_path) for file_path in all_files\n",
    ")\n",
    "\n",
    "# summary\n",
    "success = sum(1 for r in results if r[\"status\"] == \"success\")\n",
    "errors = len(all_files) - success\n",
    "\n",
    "print(\"\\n Extraction complete — results summary:\")\n",
    "print(f\" {success} succeeded, {errors} failed/skipped\")\n",
    "print(f\"Outputs saved under:\\n- Text: {processed_text_dir}\\n- Images: {processed_images_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62fbbb27-7c1c-4204-ad46-f3788c0ace92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rispy in ./.local/lib/python3.12/site-packages (0.10.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rispy --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af047b08-5887-42ba-a52d-c0fb4d6ada5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>year</th>\n",
       "      <th>doi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phd.ris</td>\n",
       "      <td>Regional Registration of Whole Slide Image Sta...</td>\n",
       "      <td>[Paknezhad, Mahsa, Loh, Sheng Yang Michael, Ch...</td>\n",
       "      <td>2020</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phd.ris</td>\n",
       "      <td>Deep Learning on Multimodal Chemical and Whole...</td>\n",
       "      <td>[Haque, Md Inzamam Ul, Mukherjee, Debangshu, S...</td>\n",
       "      <td>2022</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phd.ris</td>\n",
       "      <td>3D non-rigid registration by gradient descent ...</td>\n",
       "      <td>[Cachier, P., Pennec, X.]</td>\n",
       "      <td>2000</td>\n",
       "      <td>10.1109/MMBIA.2000.852376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phd.ris</td>\n",
       "      <td>Chemo-informatic strategy for imaging mass spe...</td>\n",
       "      <td>[Veselkov, Kirill A., Mirnezami, Reza, Strittm...</td>\n",
       "      <td>2014</td>\n",
       "      <td>10.1073/pnas.1310524111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phd.ris</td>\n",
       "      <td>A Co-registration Pipeline for Multimodal MALD...</td>\n",
       "      <td>[Nikitina, Arina, Huang, Danning, Li, Li, Pete...</td>\n",
       "      <td>2020</td>\n",
       "      <td>10.1021/jasms.9b00094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      file                                              title  \\\n",
       "0  phd.ris  Regional Registration of Whole Slide Image Sta...   \n",
       "1  phd.ris  Deep Learning on Multimodal Chemical and Whole...   \n",
       "2  phd.ris  3D non-rigid registration by gradient descent ...   \n",
       "3  phd.ris  Chemo-informatic strategy for imaging mass spe...   \n",
       "4  phd.ris  A Co-registration Pipeline for Multimodal MALD...   \n",
       "\n",
       "                                             authors  year  \\\n",
       "0  [Paknezhad, Mahsa, Loh, Sheng Yang Michael, Ch...  2020   \n",
       "1  [Haque, Md Inzamam Ul, Mukherjee, Debangshu, S...  2022   \n",
       "2                          [Cachier, P., Pennec, X.]  2000   \n",
       "3  [Veselkov, Kirill A., Mirnezami, Reza, Strittm...  2014   \n",
       "4  [Nikitina, Arina, Huang, Danning, Li, Li, Pete...  2020   \n",
       "\n",
       "                         doi  \n",
       "0                       None  \n",
       "1                       None  \n",
       "2  10.1109/MMBIA.2000.852376  \n",
       "3    10.1073/pnas.1310524111  \n",
       "4      10.1021/jasms.9b00094  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rispy\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "ris_dir = Path(\"/courses/DS5500.202610/data/team1/raw_data/sai\")  # adjust\n",
    "ris_files = list(ris_dir.rglob(\"*.ris\"))\n",
    "\n",
    "records = []\n",
    "for ris_file in ris_files:\n",
    "    with open(ris_file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        entries = rispy.load(f)\n",
    "        for entry in entries:\n",
    "            records.append({\n",
    "                \"file\": ris_file.name,\n",
    "                \"title\": entry.get(\"title\"),\n",
    "                \"authors\": entry.get(\"authors\"),\n",
    "                \"year\": entry.get(\"year\"),\n",
    "                \"doi\": entry.get(\"doi\")  \n",
    "            })\n",
    "\n",
    "ris_df = pd.DataFrame(records)\n",
    "ris_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8c15c46-96e4-462e-ad05-2aedcf8ed379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ris_df['title'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02d8ab90-111a-4d51-8dee-52f651ade1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzywuzzy[speedup] in ./.local/lib/python3.12/site-packages (0.18.0)\n",
      "Requirement already satisfied: python-levenshtein>=0.12 in ./.local/lib/python3.12/site-packages (from fuzzywuzzy[speedup]) (0.27.1)\n",
      "Requirement already satisfied: Levenshtein==0.27.1 in ./.local/lib/python3.12/site-packages (from python-levenshtein>=0.12->fuzzywuzzy[speedup]) (0.27.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in ./.local/lib/python3.12/site-packages (from Levenshtein==0.27.1->python-levenshtein>=0.12->fuzzywuzzy[speedup]) (3.14.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fuzzywuzzy[speedup] --user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24fb46d0-0b9c-47a3-a676-3d66719f493e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Removed 0 duplicate titles from RIS data (145 unique titles remain).\n",
      "Found 244 extracted text files.\n",
      "\n",
      "\n",
      " Mapping complete!\n",
      "122 matched files (≥70 similarity)\n",
      "Results saved to: /home/anbarasan.p/matched_metadata.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>matched_title</th>\n",
       "      <th>similarity</th>\n",
       "      <th>doi</th>\n",
       "      <th>year</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0 Image Gradients and Gradient Filtering.txt</td>\n",
       "      <td>4.0 Image Gradients and Gradient Filtering</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abdelmoula et al. - 2014 - Automatic Registrat...</td>\n",
       "      <td>Automatic Registration of Mass Spectrometry Im...</td>\n",
       "      <td>74.683544</td>\n",
       "      <td>10.1021/ac500148a</td>\n",
       "      <td>2014</td>\n",
       "      <td>Abdelmoula, Walid M.; Carreira, Ricardo J.; Sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alexandrov - 2012 - MALDI imaging mass spectro...</td>\n",
       "      <td>MALDI imaging mass spectrometry: statistical d...</td>\n",
       "      <td>70.886076</td>\n",
       "      <td>10.1186/1471-2105-13-S16-S11</td>\n",
       "      <td>2012</td>\n",
       "      <td>Alexandrov, Theodore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alexandrov - 2012 - MALDI imaging mass spectro...</td>\n",
       "      <td>MALDI imaging mass spectrometry: statistical d...</td>\n",
       "      <td>92.156863</td>\n",
       "      <td>10.1186/1471-2105-13-S16-S11</td>\n",
       "      <td>2012</td>\n",
       "      <td>Alexandrov, Theodore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alexandrov et al. - 2023 - Enablers and challe...</td>\n",
       "      <td>Enablers and challenges of spatial omics, a me...</td>\n",
       "      <td>86.419753</td>\n",
       "      <td>10.15252/msb.202110571</td>\n",
       "      <td>2023</td>\n",
       "      <td>Alexandrov, Theodore; Saez‐Rodriguez, Julio; S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                file  \\\n",
       "0     4.0 Image Gradients and Gradient Filtering.txt   \n",
       "1  Abdelmoula et al. - 2014 - Automatic Registrat...   \n",
       "2  Alexandrov - 2012 - MALDI imaging mass spectro...   \n",
       "3  Alexandrov - 2012 - MALDI imaging mass spectro...   \n",
       "4  Alexandrov et al. - 2023 - Enablers and challe...   \n",
       "\n",
       "                                       matched_title  similarity  \\\n",
       "0         4.0 Image Gradients and Gradient Filtering  100.000000   \n",
       "1  Automatic Registration of Mass Spectrometry Im...   74.683544   \n",
       "2  MALDI imaging mass spectrometry: statistical d...   70.886076   \n",
       "3  MALDI imaging mass spectrometry: statistical d...   92.156863   \n",
       "4  Enablers and challenges of spatial omics, a me...   86.419753   \n",
       "\n",
       "                            doi  year  \\\n",
       "0                          None  None   \n",
       "1             10.1021/ac500148a  2014   \n",
       "2  10.1186/1471-2105-13-S16-S11  2012   \n",
       "3  10.1186/1471-2105-13-S16-S11  2012   \n",
       "4        10.15252/msb.202110571  2023   \n",
       "\n",
       "                                             authors  \n",
       "0                                               None  \n",
       "1  Abdelmoula, Walid M.; Carreira, Ricardo J.; Sh...  \n",
       "2                               Alexandrov, Theodore  \n",
       "3                               Alexandrov, Theodore  \n",
       "4  Alexandrov, Theodore; Saez‐Rodriguez, Julio; S...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from rapidfuzz import process, fuzz\n",
    "import re\n",
    "\n",
    "# path\n",
    "processed_text_dir = Path(\"/home/anbarasan.p/processed_text\")  # folder with extracted text files\n",
    "mapping_output = Path(\"/home/anbarasan.p/matched_metadata.csv\")\n",
    "\n",
    "# cleaning function\n",
    "def clean_text(s):\n",
    "    \"\"\"Normalize strings by lowercasing, removing punctuation, and condensing spaces.\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r'[^a-z0-9\\s]', '', s)\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "# cleanup RIS data\n",
    "ris_df[\"clean_title\"] = ris_df[\"title\"].apply(clean_text)\n",
    "before = len(ris_df)\n",
    "ris_df = ris_df.drop_duplicates(subset=[\"clean_title\"], keep=\"first\").reset_index(drop=True)\n",
    "after = len(ris_df)\n",
    "print(f\" Removed {before - after} duplicate titles from RIS data ({after} unique titles remain).\")\n",
    "\n",
    "# prep data\n",
    "ris_titles = ris_df[\"clean_title\"].tolist()\n",
    "text_files = list(processed_text_dir.glob(\"*.txt\"))\n",
    "print(f\"Found {len(text_files)} extracted text files.\\n\")\n",
    "\n",
    "records = []\n",
    "\n",
    "# fuzzy matching\n",
    "for text_file in text_files:\n",
    "    fname = text_file.stem\n",
    "    clean_fname = clean_text(fname)\n",
    "\n",
    "    # Match filename to closest RIS title\n",
    "    match, score, idx = process.extractOne(clean_fname, ris_titles, scorer=fuzz.token_sort_ratio)\n",
    "\n",
    "    # Keep only good matches (≥70 similarity)\n",
    "    if score >= 70:\n",
    "        matched_row = ris_df.iloc[idx]\n",
    "        records.append({\n",
    "            \"file\": text_file.name,\n",
    "            \"matched_title\": matched_row[\"title\"],\n",
    "            \"similarity\": score,\n",
    "            \"doi\": matched_row.get(\"doi\"),\n",
    "            \"year\": matched_row.get(\"year\"),\n",
    "            \"authors\": \"; \".join(matched_row.get(\"authors\", [])) if isinstance(matched_row.get(\"authors\"), list) else matched_row.get(\"authors\")\n",
    "        })\n",
    "\n",
    "# export results\n",
    "mappings_df = pd.DataFrame(records)\n",
    "mappings_df.to_csv(mapping_output, index=False)\n",
    "\n",
    "# summary\n",
    "print(f\"\\n Mapping complete!\")\n",
    "print(f\"{len(mappings_df)} matched files (≥70 similarity)\")\n",
    "print(f\"Results saved to: {mapping_output}\")\n",
    "\n",
    "mappings_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d69195-5ff2-4d06-9b45-29e2e169cf86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
