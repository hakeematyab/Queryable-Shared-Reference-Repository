{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27f7333b-7f1d-41aa-b785-100683e862fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cache_dir ='/scratch/hakeem.at/Queryable-Shared-Reference-Repository/notebooks/pretrained_models'\n",
    "\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = cache_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73747be1-686e-4188-813c-d0729b81c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "logging.getLogger(\"docling\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"docling.backend\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"docling.datamodel\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"docling_parse\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"PIL\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"pdfplumber\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\n",
    "\n",
    "for logger_name in logging.Logger.manager.loggerDict.keys():\n",
    "    if 'docling' in logger_name.lower() or 'pdf' in logger_name.lower():\n",
    "        logging.getLogger(logger_name).setLevel(logging.CRITICAL)\n",
    "\n",
    "@contextmanager\n",
    "def suppress_stdout_stderr():\n",
    "    \"\"\"Suppress all output to stdout and stderr.\"\"\"\n",
    "    null_file = open(os.devnull, 'w')\n",
    "    old_stdout = sys.stdout\n",
    "    old_stderr = sys.stderr\n",
    "    sys.stdout = null_file\n",
    "    sys.stderr = null_file\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "        sys.stderr = old_stderr\n",
    "        null_file.close()\n",
    "\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7db717e9-3c3d-4c98-a553-7e83df4c929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with suppress_stdout_stderr():\n",
    "    print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aedea906-a596-4697-9fdc-f391bc2fc120",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, BSHTMLLoader, Docx2txtLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModel\n",
    "# from adapters import AutoAdapterModel\n",
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14974f42-9ebb-4ad7-a07a-4affa97ef7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.backend.docling_parse_v2_backend  import DoclingParseV2DocumentBackend\n",
    "from docling_core.types.doc.document import DoclingDocument\n",
    "from docling.chunking import HybridChunker\n",
    "from docling_core.transforms.chunker.hierarchical_chunker import HierarchicalChunker\n",
    "from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer\n",
    "from docling_core.types.doc.labels import DocItemLabel\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf1a6675-2649-454c-99e6-2b01d2b1493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mistune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "999b4dd1-7565-4294-81a6-14720b0666a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.do_ocr = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82d489c6-864c-44f1-8216-5ee8c20180e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def clear_gpu(*items):\n",
    "    for item in items:\n",
    "        del item\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cde29e3-bb2b-44b2-ab95-71bed69d2b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7d92758f-c433-429b-84fc-8948df211443",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = DocumentConverter(format_options={\n",
    "    InputFormat.PDF: PdfFormatOption(\n",
    "        pipeline_options = pipeline_options,\n",
    "        backend = DoclingParseV2DocumentBackend\n",
    "    )\n",
    "})\n",
    "chunker = HybridChunker(merge_peers=True, max_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77882bc5-d1ba-4a88-a3ce-ecb8d60f5e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"docling_processed_text.jsonl\"\n",
    "loaded_docs = []\n",
    "with open(output_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        doc = json.loads(line)\n",
    "        doc = DoclingDocument.model_validate(doc)\n",
    "        loaded_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fd3ae76b-ba69-470f-bdbe-15ad48a2832a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68655b9fca8b4f3ea38800d250ef9389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "chunk_lookup = defaultdict(dict)\n",
    "\n",
    "min_chunk_length = 100\n",
    "skip_headings = ['reference','references', 'bibliography', 'works cited', 'citations']\n",
    "for doc_idx, loaded_doc in enumerate(tqdm(loaded_docs, total = len(loaded_docs))):\n",
    "    chunks = list(chunker.chunk(dl_doc = loaded_doc))\n",
    "    for chunk_idx, chunk in enumerate(chunks):\n",
    "        if not len(chunk.text.strip()) > min_chunk_length:\n",
    "            continue\n",
    "        metadata = chunk.meta\n",
    "        skip = False\n",
    "        if hasattr(metadata, \"headings\") and metadata.headings:\n",
    "            for heading in metadata.headings:\n",
    "                if heading.lower() in skip_headings:\n",
    "                    skip=True\n",
    "                    break\n",
    "        if skip:\n",
    "            continue\n",
    "        chunked_data = {\n",
    "            'chunk': chunk.text.strip(),\n",
    "            'doc_idx': doc_idx,\n",
    "            'chunk_idx':chunk_idx,\n",
    "            'filename':loaded_doc.origin.filename\n",
    "        }\n",
    "        if chunked_data[\"chunk\"]:\n",
    "            text_key = chunked_data[\"chunk\"][:200].strip()\n",
    "            chunk_lookup[chunked_data['filename']][text_key] = (chunked_data['chunk_idx'], chunked_data['chunk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19eefaba-516b-4a1c-ba90-036d32df0266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>question_type</th>\n",
       "      <th>context</th>\n",
       "      <th>source</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>prompt_type</th>\n",
       "      <th>raw_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>What is the DOI for the Supporting Information...</td>\n",
       "      <td>answerable</td>\n",
       "      <td>The Supporting Information is available free o...</td>\n",
       "      <td>Holzlechner et al. - 2017 - In Situ Characteri...</td>\n",
       "      <td>The Supporting Information is available free o...</td>\n",
       "      <td>explicit_idk</td>\n",
       "      <td>The DOI for the Supporting Information is 10.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>What software was used to process the MALDI MS...</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>The Supporting Information is available free o...</td>\n",
       "      <td>Holzlechner et al. - 2017 - In Situ Characteri...</td>\n",
       "      <td>None</td>\n",
       "      <td>explicit_idk</td>\n",
       "      <td>I don't know.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>What method was used to measure local variance...</td>\n",
       "      <td>answerable</td>\n",
       "      <td>Table /1 the derivative of the sum of local co...</td>\n",
       "      <td>Cachier and Pennec - 2000 - 3D non-rigid regis...</td>\n",
       "      <td>cal variance using the difference of mean rela...</td>\n",
       "      <td>explicit_idk</td>\n",
       "      <td>The method used to measure local variance usin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>What specific programming language was used to...</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>Table /1 the derivative of the sum of local co...</td>\n",
       "      <td>Cachier and Pennec - 2000 - 3D non-rigid regis...</td>\n",
       "      <td>None</td>\n",
       "      <td>explicit_idk</td>\n",
       "      <td>I don't know.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>What cell lines were obtained from American Ty...</td>\n",
       "      <td>answerable</td>\n",
       "      <td>For details about the chemicals used in the st...</td>\n",
       "      <td>Zhang et al. - 2023 - Single-cell lipidomics e...</td>\n",
       "      <td>The commercial human cell lines, including pan...</td>\n",
       "      <td>explicit_idk</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question question_type  \\\n",
       "1500  What is the DOI for the Supporting Information...    answerable   \n",
       "1502  What software was used to process the MALDI MS...  unanswerable   \n",
       "1503  What method was used to measure local variance...    answerable   \n",
       "1505  What specific programming language was used to...  unanswerable   \n",
       "1506  What cell lines were obtained from American Ty...    answerable   \n",
       "\n",
       "                                                context  \\\n",
       "1500  The Supporting Information is available free o...   \n",
       "1502  The Supporting Information is available free o...   \n",
       "1503  Table /1 the derivative of the sum of local co...   \n",
       "1505  Table /1 the derivative of the sum of local co...   \n",
       "1506  For details about the chemicals used in the st...   \n",
       "\n",
       "                                                 source  \\\n",
       "1500  Holzlechner et al. - 2017 - In Situ Characteri...   \n",
       "1502  Holzlechner et al. - 2017 - In Situ Characteri...   \n",
       "1503  Cachier and Pennec - 2000 - 3D non-rigid regis...   \n",
       "1505  Cachier and Pennec - 2000 - 3D non-rigid regis...   \n",
       "1506  Zhang et al. - 2023 - Single-cell lipidomics e...   \n",
       "\n",
       "                                           ground_truth   prompt_type  \\\n",
       "1500  The Supporting Information is available free o...  explicit_idk   \n",
       "1502                                               None  explicit_idk   \n",
       "1503  cal variance using the difference of mean rela...  explicit_idk   \n",
       "1505                                               None  explicit_idk   \n",
       "1506  The commercial human cell lines, including pan...  explicit_idk   \n",
       "\n",
       "                                           raw_response  \n",
       "1500  The DOI for the Supporting Information is 10.1...  \n",
       "1502                                      I don't know.  \n",
       "1503  The method used to measure local variance usin...  \n",
       "1505                                      I don't know.  \n",
       "1506                                                     "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_dp = 900\n",
    "df = pd.read_json(\"prompt_thresholding_responses.jsonl\")\n",
    "df = df[(df['question_type']!='borderline') & (df['prompt_type']==\"explicit_idk\")]\n",
    "df = df[:900]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b32a6c7-d320-45e0-a351-406482c64642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b14295f0e194393862030e89d20fc1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match rate: 900/900 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "matched = 0\n",
    "unmatched_examples = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    filename = row['source']\n",
    "    context = row['context'][:200].strip()\n",
    "    \n",
    "    if filename in chunk_lookup and context in chunk_lookup[filename]:\n",
    "        matched += 1\n",
    "    else:\n",
    "        unmatched_examples.append({\n",
    "            'idx': idx,\n",
    "            'filename': filename,\n",
    "            'context_len': len(context),\n",
    "            'context_preview': context[:100]\n",
    "        })\n",
    "\n",
    "print(f\"Exact match rate: {matched}/{len(df)} ({matched*100/len(df):.1f}%)\")\n",
    "\n",
    "if unmatched_examples:\n",
    "    print(f\"\\nFirst unmatched example:\")\n",
    "    ex = unmatched_examples[0]\n",
    "    print(f\"Filename: {ex['filename']}\")\n",
    "    print(f\"Context preview: {ex['context_preview']}\")\n",
    "    \n",
    "    if ex['filename'] in chunk_lookup:\n",
    "        print(f\"Filename found, {len(chunk_lookup[ex['filename']])} chunks available\")\n",
    "        for key in list(chunk_lookup[ex['filename']].keys())[:3]:\n",
    "            print(f\"Sample chunk start: {key[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4ac56717-bc6e-49e5-9e9d-3d58393f0d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2e6e44e3-3ba8-4613-9b72-b596cab772a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_top(filename, current_idx, target_tokens):\n",
    "    current_tokens, current_text = chunk_data[filename][current_idx]\n",
    "    \n",
    "    if current_tokens >= target_tokens:\n",
    "        return current_text, current_tokens\n",
    "    \n",
    "    sorted_idx = chunk_indices[filename]\n",
    "    current_pos = sorted_idx.index(current_idx)\n",
    "    \n",
    "    indices_before = list(reversed(sorted_idx[:current_pos]))\n",
    "    \n",
    "    expanded_chunks = []\n",
    "    accumulated_tokens = 0\n",
    "    needed_tokens = target_tokens - current_tokens\n",
    "    \n",
    "    for idx in indices_before:\n",
    "        if accumulated_tokens >= needed_tokens:\n",
    "            break\n",
    "        num_tokens, text = chunk_data[filename][idx]\n",
    "        expanded_chunks.append((idx, text, num_tokens))\n",
    "        accumulated_tokens += num_tokens\n",
    "    \n",
    "    if accumulated_tokens < needed_tokens and expanded_chunks:\n",
    "        while accumulated_tokens < needed_tokens:\n",
    "            for idx, text, num_tokens in list(expanded_chunks):\n",
    "                if accumulated_tokens >= needed_tokens:\n",
    "                    break\n",
    "                expanded_chunks.append((idx, text, num_tokens))\n",
    "                accumulated_tokens += num_tokens\n",
    "    \n",
    "    expanded_chunks.sort(key=lambda x: x[0])\n",
    "    \n",
    "    padding_text = \"\\n\\n\".join([text for _, text, _ in expanded_chunks])\n",
    "    final_text = padding_text + \"\\n\\n\" + current_text if padding_text else current_text\n",
    "    \n",
    "    return final_text, accumulated_tokens + current_tokens\n",
    "\n",
    "\n",
    "def expand_bottom(filename, current_idx, target_tokens):\n",
    "    current_tokens, current_text = chunk_data[filename][current_idx]\n",
    "    \n",
    "    if current_tokens >= target_tokens:\n",
    "        return current_text, current_tokens\n",
    "    \n",
    "    sorted_idx = chunk_indices[filename]\n",
    "    current_pos = sorted_idx.index(current_idx)\n",
    "    \n",
    "    indices_after = sorted_idx[current_pos + 1:]\n",
    "    \n",
    "    expanded_chunks = []\n",
    "    accumulated_tokens = 0\n",
    "    needed_tokens = target_tokens - current_tokens\n",
    "    \n",
    "    for idx in indices_after:\n",
    "        if accumulated_tokens >= needed_tokens:\n",
    "            break\n",
    "        num_tokens, text = chunk_data[filename][idx]\n",
    "        expanded_chunks.append((idx, text, num_tokens))\n",
    "        accumulated_tokens += num_tokens\n",
    "    \n",
    "    if accumulated_tokens < needed_tokens and expanded_chunks:\n",
    "        while accumulated_tokens < needed_tokens:\n",
    "            for idx, text, num_tokens in list(expanded_chunks):\n",
    "                if accumulated_tokens >= needed_tokens:\n",
    "                    break\n",
    "                expanded_chunks.append((idx, text, num_tokens))\n",
    "                accumulated_tokens += num_tokens\n",
    "    \n",
    "    padding_text = \"\\n\\n\".join([text for _, text, _ in expanded_chunks])\n",
    "    final_text = current_text + \"\\n\\n\" + padding_text if padding_text else current_text\n",
    "    \n",
    "    return final_text, accumulated_tokens + current_tokens\n",
    "\n",
    "\n",
    "def expand_middle(filename, current_idx, target_tokens):\n",
    "    current_tokens, current_text = chunk_data[filename][current_idx]\n",
    "    \n",
    "    if current_tokens >= target_tokens:\n",
    "        return current_text, current_tokens\n",
    "    \n",
    "    sorted_idx = chunk_indices[filename]\n",
    "    current_pos = sorted_idx.index(current_idx)\n",
    "    \n",
    "    indices_before = list(reversed(sorted_idx[:current_pos]))\n",
    "    indices_after = sorted_idx[current_pos + 1:]\n",
    "    \n",
    "    needed_tokens = target_tokens - current_tokens\n",
    "    needed_per_side = needed_tokens // 2\n",
    "    \n",
    "    before_chunks = []\n",
    "    after_chunks = []\n",
    "    before_tokens = 0\n",
    "    after_tokens = 0\n",
    "    \n",
    "    for idx in indices_before:\n",
    "        if before_tokens >= needed_per_side:\n",
    "            break\n",
    "        num_tokens, text = chunk_data[filename][idx]\n",
    "        before_chunks.append((idx, text, num_tokens))\n",
    "        before_tokens += num_tokens\n",
    "    \n",
    "    for idx in indices_after:\n",
    "        if after_tokens >= needed_per_side:\n",
    "            break\n",
    "        num_tokens, text = chunk_data[filename][idx]\n",
    "        after_chunks.append((idx, text, num_tokens))\n",
    "        after_tokens += num_tokens\n",
    "    \n",
    "    total = before_tokens + after_tokens\n",
    "    if total < needed_tokens:\n",
    "        remaining = needed_tokens - total\n",
    "        \n",
    "        for idx in indices_before[len(before_chunks):]:\n",
    "            if before_tokens + after_tokens >= needed_tokens:\n",
    "                break\n",
    "            num_tokens, text = chunk_data[filename][idx]\n",
    "            before_chunks.append((idx, text, num_tokens))\n",
    "            before_tokens += num_tokens\n",
    "        \n",
    "        for idx in indices_after[len(after_chunks):]:\n",
    "            if before_tokens + after_tokens >= needed_tokens:\n",
    "                break\n",
    "            num_tokens, text = chunk_data[filename][idx]\n",
    "            after_chunks.append((idx, text, num_tokens))\n",
    "            after_tokens += num_tokens\n",
    "    \n",
    "    total = before_tokens + after_tokens\n",
    "    if total < needed_tokens:\n",
    "        all_chunks = before_chunks + after_chunks\n",
    "        if all_chunks:\n",
    "            while before_tokens + after_tokens < needed_tokens:\n",
    "                for idx, text, num_tokens in list(all_chunks):\n",
    "                    if before_tokens + after_tokens >= needed_tokens:\n",
    "                        break\n",
    "                    if idx < current_idx:\n",
    "                        before_chunks.append((idx, text, num_tokens))\n",
    "                        before_tokens += num_tokens\n",
    "                    else:\n",
    "                        after_chunks.append((idx, text, num_tokens))\n",
    "                        after_tokens += num_tokens\n",
    "    \n",
    "    before_chunks.sort(key=lambda x: x[0])\n",
    "    \n",
    "    parts = []\n",
    "    if before_chunks:\n",
    "        parts.append(\"\\n\\n\".join([text for _, text, _ in before_chunks]))\n",
    "    parts.append(current_text)\n",
    "    if after_chunks:\n",
    "        parts.append(\"\\n\\n\".join([text for _, text, _ in after_chunks]))\n",
    "    \n",
    "    final_text = \"\\n\\n\".join(parts)\n",
    "    return final_text, before_tokens + current_tokens + after_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03e52c6-3a1e-478e-afed-d696ec19e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONTEXT = 32768\n",
    "\n",
    "CONTEXT_PERCENTAGES = [0.10, 0.25, 0.50, 0.75, 0.95]\n",
    "TARGET_LENGTHS = {p: int(MAX_CONTEXT * p) for p in CONTEXT_PERCENTAGES}\n",
    "print(\"Target lengths:\", TARGET_LENGTHS)\n",
    "# {0.1: 3276, 0.25: 8192, 0.5: 16384, 0.75: 24576, 0.95: 31129}\n",
    "\n",
    "chunk_data = defaultdict(dict)    \n",
    "chunk_indices = defaultdict(list) \n",
    "\n",
    "min_chunk_length = 100\n",
    "skip_headings = ['reference', 'references', 'bibliography', 'works cited', 'citations']\n",
    "\n",
    "for doc_idx, loaded_doc in enumerate(tqdm(loaded_docs, total=len(loaded_docs))):\n",
    "    filename = loaded_doc.origin.filename\n",
    "    chunks = list(chunker.chunk(dl_doc=loaded_doc))\n",
    "    \n",
    "    for chunk_idx, chunk in enumerate(chunks):\n",
    "        text = chunk.text.strip()\n",
    "        if len(text) <= min_chunk_length:\n",
    "            continue\n",
    "            \n",
    "        metadata = chunk.meta\n",
    "        skip = False\n",
    "        if hasattr(metadata, \"headings\") and metadata.headings:\n",
    "            for heading in metadata.headings:\n",
    "                if heading.lower() in skip_headings:\n",
    "                    skip = True\n",
    "                    break\n",
    "        if skip:\n",
    "            continue\n",
    "        \n",
    "        num_tokens = len(tokenizer.encode(text, add_special_tokens=False))\n",
    "        chunk_data[filename][chunk_idx] = (num_tokens, text)\n",
    "    \n",
    "    chunk_indices[filename] = sorted(chunk_data[filename].keys())\n",
    "\n",
    "df_to_chunk = {}\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    filename = row['source']\n",
    "    context_key = row['context'][:200].strip()\n",
    "    \n",
    "    for chunk_idx, (num_tokens, text) in chunk_data[filename].items():\n",
    "        if text[:200].strip() == context_key:\n",
    "            df_to_chunk[idx] = (filename, chunk_idx)\n",
    "            break\n",
    "\n",
    "print(f\"Mapped {len(df_to_chunk)}/{len(df)} rows to chunk indices\")\n",
    "\n",
    "expansion_functions = {\n",
    "    'top': expand_top,\n",
    "    'bottom': expand_bottom,\n",
    "    'middle': expand_middle\n",
    "}\n",
    "\n",
    "df['expansion_type'] = np.tile(['top', 'bottom', 'middle'], len(df) // 3 + 1)[:len(df)]\n",
    "np.random.shuffle(df['expansion_type'].values) \n",
    "\n",
    "expanded_dataset = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if idx not in df_to_chunk:\n",
    "        continue\n",
    "    \n",
    "    filename, chunk_idx = df_to_chunk[idx]\n",
    "    expansion_type = row['expansion_type']\n",
    "    expand_fn = expansion_functions[expansion_type]\n",
    "    \n",
    "    for pct, target_len in TARGET_LENGTHS.items():\n",
    "        expanded_text, actual_tokens = expand_fn(filename, chunk_idx, target_len)\n",
    "        \n",
    "        expanded_dataset.append({\n",
    "            'original_idx': idx,\n",
    "            'source': filename,\n",
    "            'question_type': row['question_type'],\n",
    "            'query': row[\"question\"],\n",
    "            'original_context': row['context'],\n",
    "            'expanded_context': expanded_text,\n",
    "            'expansion_type': expansion_type,\n",
    "            'target_pct': pct,\n",
    "            'actual_pct': actual_tokens/MAX_CONTEXT,\n",
    "            'target_tokens': target_len,\n",
    "            'actual_tokens': actual_tokens,\n",
    "            'ground_truth': row['ground_truth'],\n",
    "            'prompt_type': row['prompt_type'],\n",
    "            'raw_response': row['raw_response'],\n",
    "        })\n",
    "\n",
    "expanded_df = pd.DataFrame(expanded_dataset)\n",
    "print(f\"Created {len(expanded_df)} expanded samples\")\n",
    "print(f\"Distribution:\\n{expanded_df.groupby(['expansion_type', 'target_pct']).size().unstack()}\")\n",
    "\n",
    "expanded_df.to_json(\"context_expansion_dataset.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2e2b4b99-1655-40da-a442-0987de944cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 16384, Recorded: 17118, Verified: 17126\n",
      "                 count          mean           std     min      25%      50%  \\\n",
      "expansion_type                                                                 \n",
      "bottom          1500.0  16422.755333  10524.443741    41.0  8227.25  16560.5   \n",
      "middle          1495.0  17152.258863  10169.104983  3279.0  8346.50  16705.0   \n",
      "top             1495.0  16316.797324  10581.560186    20.0  8229.50  16600.0   \n",
      "\n",
      "                    75%      max  \n",
      "expansion_type                    \n",
      "bottom          25034.0  32237.0  \n",
      "middle          25144.0  32293.0  \n",
      "top             25056.0  32627.0  \n"
     ]
    }
   ],
   "source": [
    "sample = expanded_df[expanded_df['target_pct'] == 0.5].iloc[0]\n",
    "actual = len(tokenizer.encode(sample['expanded_context'], add_special_tokens=False))\n",
    "print(f\"Target: {sample['target_tokens']}, Recorded: {sample['actual_tokens']}, Verified: {actual}\")\n",
    "\n",
    "print(expanded_df.groupby('expansion_type')['actual_tokens'].describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-QSRR2.0",
   "language": "python",
   "name": "qsrr2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
