{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727d4c12-6aff-40b2-ba76-b3617045964d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded 723 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 723/723 [48:09<00:00,  4.00s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Saved 722 silver answers to /home/anbarasan.p/rag_eval_silver_answers.json\n",
      "NOT_FOUND after fallback: 9/722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json, os, torch, time\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# ---------- Device ----------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# ---------- Paths ----------\n",
    "BASE_DATA_PATH = \"/courses/DS5500.202610/data/team1/processed_data\"\n",
    "RAG_EVAL_BASELINE_PATH = os.path.join(BASE_DATA_PATH, \"rag_eval_dataset.json\")\n",
    "\n",
    "# Save in your home dir (you have permission)\n",
    "SILVER_ANSWERS_PATH = \"/home/anbarasan.p/rag_eval_silver_answers.json\"\n",
    "\n",
    "# ---------- Label model ----------\n",
    "LABEL_MODEL_NAME = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(LABEL_MODEL_NAME)\n",
    "label_model = AutoModelForSeq2SeqLM.from_pretrained(LABEL_MODEL_NAME).to(DEVICE)\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(obj, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "def build_context(excerpt, retrieved_docs, max_docs=6):\n",
    "    \"\"\"\n",
    "    Combine excerpt + top retrieved docs.\n",
    "    max_docs keeps context smaller & less noisy.\n",
    "    \"\"\"\n",
    "    docs = retrieved_docs if isinstance(retrieved_docs, list) else [str(retrieved_docs)]\n",
    "    docs = docs[:max_docs]\n",
    "    context_parts = []\n",
    "    if excerpt:\n",
    "        context_parts.append(\"EXCERPT:\\n\" + excerpt)\n",
    "    if docs:\n",
    "        context_parts.append(\"RETRIEVED DOCS:\\n\" + \"\\n\\n\".join(docs))\n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "def make_prompt(question, context):\n",
    "    return f\"\"\"\n",
    "You are labeling scientific QA data.\n",
    "Answer the question using ONLY the context.\n",
    "Give a short, precise answer (1 sentence or a value).\n",
    "If the answer is not in context, respond exactly: NOT_FOUND.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_one(question, context):\n",
    "    prompt = make_prompt(question, context)\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=2048  # allow longer input\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    outputs = label_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=80,   # short answer\n",
    "        do_sample=False\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "# ---------- Generate silver labels ----------\n",
    "records = load_json(RAG_EVAL_BASELINE_PATH)\n",
    "print(f\"Loaded {len(records)} records.\")\n",
    "\n",
    "silver_answers = {}\n",
    "not_found_count = 0\n",
    "\n",
    "for rec in tqdm(records):\n",
    "    q = rec.get(\"question\")\n",
    "    excerpt = rec.get(\"excerpt\", \"\")\n",
    "    docs = rec.get(\"retrieved_docs\", [])\n",
    "    if not q:\n",
    "        continue\n",
    "\n",
    "    # pass 1: excerpt + top docs\n",
    "    context = build_context(excerpt, docs, max_docs=6)\n",
    "    ans = generate_one(q, context)\n",
    "\n",
    "    # pass 2 fallback: excerpt only\n",
    "    if ans.strip().upper() == \"NOT_FOUND\":\n",
    "        ans = generate_one(q, excerpt)\n",
    "        if ans.strip().upper() == \"NOT_FOUND\":\n",
    "            not_found_count += 1\n",
    "\n",
    "    silver_answers[q] = ans\n",
    "\n",
    "save_json(silver_answers, SILVER_ANSWERS_PATH)\n",
    "print(f\"\\nâœ… Saved {len(silver_answers)} silver answers to {SILVER_ANSWERS_PATH}\")\n",
    "print(f\"NOT_FOUND after fallback: {not_found_count}/{len(silver_answers)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ab780cf-c5a7-4376-9e62-520ff57006da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 23 17:13:27 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:41:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             51W /  400W |       1MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-40GB          On  |   00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             49W /  400W |       1MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b4d5245-5a7d-4510-9f9b-01b947ebf2fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "âœ… bitsandbytes found -> loading Qwen3-8B in 4-bit.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e35f5d29344d92b36ced9028c8b849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline records: 723\n",
      "Docling records:  723\n",
      "Silver answers:   722\n",
      "\n",
      "ðŸ“Œ Found existing predictions at /home/anbarasan.p/qwen3_predictions_quant.json. Resuming...\n",
      "âœ… Already completed: 2888 items\n",
      "\n",
      "=== Evaluating Qwen3-8B on baseline MiniLM ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 723/723 [00:00<00:00, 2286939.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Qwen3-8B on docling retriever: miniLM ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 723/723 [00:00<00:00, 2271521.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Qwen3-8B on docling retriever: hybrid_gemma ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 723/723 [00:00<00:00, 3542618.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Qwen3-8B on docling retriever: hybrid_gemma2048 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 723/723 [00:00<00:00, 3538485.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Final predictions saved to /home/anbarasan.p/qwen3_predictions_quant.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 341\u001b[0m\n\u001b[1;32m    337\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 341\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[1], line 288\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    275\u001b[0m     ragas_ds \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict({\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: [x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m valid],\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontexts\u001b[39m\u001b[38;5;124m\"\u001b[39m: [[context_from_docs(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: [x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m valid],\n\u001b[1;32m    284\u001b[0m     })\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# NOTE: If you want exact RAGAS contexts, store \"context\" in predictions.\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# RAGAS will still run, but faithfulness/relevancy are weaker without true contexts.\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     ragas_out \u001b[38;5;241m=\u001b[39m evaluate(ragas_ds, metrics\u001b[38;5;241m=\u001b[39m[faithfulness, answer_relevancy])\n\u001b[1;32m    290\u001b[0m     summaries\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: QWEN_MODEL_NAME,\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized_4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m: USE_BNB,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_items\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(items)\n\u001b[1;32m    303\u001b[0m     })\n\u001b[1;32m    305\u001b[0m save_json(summaries, SUMMARY_OUT)\n",
      "File \u001b[0;32m/shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages/ragas/_analytics.py:277\u001b[0m, in \u001b[0;36mtrack_was_completed.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    276\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m--> 277\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    278\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages/ragas/evaluation.py:458\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, experiment_name, callbacks, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar, return_executor, allow_nest_asyncio)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;66;03m# Default behavior: use nest_asyncio for backward compatibility (Jupyter notebooks)\u001b[39;00m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masync_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m run(_async_wrapper())\n",
      "File \u001b[0;32m/shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages/ragas/async_utils.py:125\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(async_func, allow_nest_asyncio)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Create the coroutine if it's a callable, otherwise use directly\u001b[39;00m\n\u001b[1;32m    124\u001b[0m coro \u001b[38;5;241m=\u001b[39m async_func() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(async_func) \u001b[38;5;28;01melse\u001b[39;00m async_func\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mrun(coro)\n",
      "File \u001b[0;32m/shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mrun_until_complete(task)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m/shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m/shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/asyncio/futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/asyncio/tasks.py:314\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m/shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages/ragas/evaluation.py:431\u001b[0m, in \u001b[0;36mevaluate.<locals>._async_wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_async_wrapper\u001b[39m():\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m aevaluate(\n\u001b[1;32m    432\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m    433\u001b[0m         metrics\u001b[38;5;241m=\u001b[39mmetrics,\n\u001b[1;32m    434\u001b[0m         llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m    435\u001b[0m         embeddings\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[1;32m    436\u001b[0m         experiment_name\u001b[38;5;241m=\u001b[39mexperiment_name,\n\u001b[1;32m    437\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    438\u001b[0m         run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[1;32m    439\u001b[0m         token_usage_parser\u001b[38;5;241m=\u001b[39mtoken_usage_parser,\n\u001b[1;32m    440\u001b[0m         raise_exceptions\u001b[38;5;241m=\u001b[39mraise_exceptions,\n\u001b[1;32m    441\u001b[0m         column_map\u001b[38;5;241m=\u001b[39mcolumn_map,\n\u001b[1;32m    442\u001b[0m         show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[1;32m    443\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    444\u001b[0m         _run_id\u001b[38;5;241m=\u001b[39m_run_id,\n\u001b[1;32m    445\u001b[0m         _pbar\u001b[38;5;241m=\u001b[39m_pbar,\n\u001b[1;32m    446\u001b[0m         return_executor\u001b[38;5;241m=\u001b[39mreturn_executor,\n\u001b[1;32m    447\u001b[0m     )\n",
      "File \u001b[0;32m/shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages/ragas/evaluation.py:168\u001b[0m, in \u001b[0;36maevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, experiment_name, callbacks, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar, return_executor)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(metric, MetricWithLLM) \u001b[38;5;129;01mand\u001b[39;00m metric\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m         llm \u001b[38;5;241m=\u001b[39m llm_factory()\n\u001b[1;32m    169\u001b[0m     metric\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m llm\n\u001b[1;32m    170\u001b[0m     llm_changed\u001b[38;5;241m.\u001b[39mappend(i)\n",
      "File \u001b[0;32m/shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages/ragas/llms/base.py:473\u001b[0m, in \u001b[0;36mllm_factory\u001b[0;34m(model, run_config, default_headers, base_url)\u001b[0m\n\u001b[1;32m    470\u001b[0m     default_headers \u001b[38;5;241m=\u001b[39m helicone_config\u001b[38;5;241m.\u001b[39mdefault_headers()\n\u001b[1;32m    471\u001b[0m     base_url \u001b[38;5;241m=\u001b[39m helicone_config\u001b[38;5;241m.\u001b[39mbase_url\n\u001b[0;32m--> 473\u001b[0m openai_model \u001b[38;5;241m=\u001b[39m ChatOpenAI(\n\u001b[1;32m    474\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, timeout\u001b[38;5;241m=\u001b[39mtimeout, default_headers\u001b[38;5;241m=\u001b[39mdefault_headers, base_url\u001b[38;5;241m=\u001b[39mbase_url\n\u001b[1;32m    475\u001b[0m )\n\u001b[1;32m    477\u001b[0m \u001b[38;5;66;03m# Track factory usage\u001b[39;00m\n\u001b[1;32m    478\u001b[0m track(\n\u001b[1;32m    479\u001b[0m     LLMUsageEvent(\n\u001b[1;32m    480\u001b[0m         provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m     )\n\u001b[1;32m    486\u001b[0m )\n",
      "File \u001b[0;32m/shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages/langchain_core/load/serializable.py:115\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:825\u001b[0m, in \u001b[0;36mBaseChatOpenAI.validate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mClient(\n\u001b[1;32m    819\u001b[0m             proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy, verify\u001b[38;5;241m=\u001b[39mglobal_ssl_context\n\u001b[1;32m    820\u001b[0m         )\n\u001b[1;32m    821\u001b[0m     sync_specific \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client\n\u001b[1;32m    823\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _get_default_httpx_client(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_api_base, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_timeout)\n\u001b[1;32m    824\u001b[0m     }\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mOpenAI(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclient_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msync_specific)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client:\n",
      "File \u001b[0;32m/shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages/openai/_client.py:135\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    133\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m     )\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(api_key):\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "import json, os, time, torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from ragas.metrics import faithfulness, answer_relevancy\n",
    "from ragas import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "###############################################################################\n",
    "#                             DEVICE CONFIG\n",
    "###############################################################################\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "###############################################################################\n",
    "#                             PATH CONFIG\n",
    "###############################################################################\n",
    "BASE_DATA_PATH = \"/courses/DS5500.202610/data/team1/processed_data\"\n",
    "\n",
    "RAG_EVAL_BASELINE_PATH = os.path.join(BASE_DATA_PATH, \"rag_eval_dataset.json\")\n",
    "RAG_EVAL_DOCLING_PATH  = os.path.join(BASE_DATA_PATH, \"rag_eval_dataset_docling.json\")\n",
    "\n",
    "# silver answers location (home dir)\n",
    "SILVER_ANSWERS_PATH = \"/home/anbarasan.p/rag_eval_silver_answers.json\"\n",
    "\n",
    "# outputs (home dir)\n",
    "PREDICTIONS_OUT = \"/home/anbarasan.p/qwen3_predictions_quant.json\"\n",
    "SUMMARY_OUT     = \"/home/anbarasan.p/qwen3_benchmarks_summary_quant.json\"\n",
    "\n",
    "# Docling retrievers + fields\n",
    "RETRIEVERS_DOCLING = [\"miniLM\", \"hybrid_gemma\", \"hybrid_gemma2048\"]\n",
    "RETRIEVER_TO_FIELD = {\n",
    "    \"miniLM\": \"retrieved_docs_minilm_hybrid\",\n",
    "    \"hybrid_gemma\": \"retrieved_docs_gemma_hybrid\",\n",
    "    \"hybrid_gemma2048\": \"retrieved_docs_gemma2048_hybrid\",\n",
    "}\n",
    "\n",
    "###############################################################################\n",
    "#                             MODEL CONFIG\n",
    "###############################################################################\n",
    "QWEN_MODEL_NAME = \"Qwen/Qwen3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(QWEN_MODEL_NAME)\n",
    "\n",
    "USE_BNB = False\n",
    "if DEVICE == \"cuda\":\n",
    "    try:\n",
    "        import bitsandbytes  # noqa\n",
    "        USE_BNB = True\n",
    "        print(\"âœ… bitsandbytes found -> loading Qwen3-8B in 4-bit.\")\n",
    "    except Exception:\n",
    "        print(\"âš ï¸ bitsandbytes not found -> loading full precision on GPU.\")\n",
    "else:\n",
    "    print(\"âš ï¸ CPU session detected. Will be slow.\")\n",
    "\n",
    "if USE_BNB:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        QWEN_MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        QWEN_MODEL_NAME,\n",
    "        torch_dtype=\"auto\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "###############################################################################\n",
    "#                             HELPERS\n",
    "###############################################################################\n",
    "def load_json(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(obj, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "def compute_precision_recall(pred, gold):\n",
    "    if gold is None or gold == \"\" or gold == \"N/A\":\n",
    "        return None, None\n",
    "    pred_tokens = pred.lower().split()\n",
    "    gold_tokens = gold.lower().split()\n",
    "    if not pred_tokens or not gold_tokens:\n",
    "        return None, None\n",
    "    tp = len([t for t in pred_tokens if t in gold_tokens])\n",
    "    precision = tp / len(pred_tokens)\n",
    "    recall = tp / len(gold_tokens)\n",
    "    return precision, recall\n",
    "\n",
    "def get_gpu_stats():\n",
    "    if not torch.cuda.is_available():\n",
    "        return 0.0, 0.0\n",
    "    allocated = torch.cuda.memory_allocated() / (1024**3)\n",
    "    reserved  = torch.cuda.memory_reserved() / (1024**3)\n",
    "    return float(allocated), float(reserved)\n",
    "\n",
    "def context_from_docs(docs, max_docs=3, max_chars=4000):\n",
    "    if isinstance(docs, list):\n",
    "        text = \"\\n\\n\".join(docs[:max_docs])\n",
    "    else:\n",
    "        text = str(docs)\n",
    "    return text[:max_chars]\n",
    "\n",
    "def make_prompt(question, context):\n",
    "    return f\"\"\"\n",
    "You are a scientific assistant. Answer the question using ONLY the information from the context.\n",
    "Do NOT use outside knowledge. If the answer cannot be found, say:\n",
    "\"The context does not provide enough information.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Final Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_answer(question, context):\n",
    "    prompt = make_prompt(question, context)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(DEVICE)\n",
    "\n",
    "    start = time.time()\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=False   # deterministic\n",
    "    )\n",
    "    latency = time.time() - start\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    ga, gr = get_gpu_stats()\n",
    "    return answer, latency, ga, gr\n",
    "\n",
    "###############################################################################\n",
    "#                  RESUME LOGIC: LOAD EXISTING PREDICTIONS\n",
    "###############################################################################\n",
    "def load_existing_predictions():\n",
    "    if os.path.exists(PREDICTIONS_OUT):\n",
    "        print(f\"\\nðŸ“Œ Found existing predictions at {PREDICTIONS_OUT}. Resuming...\")\n",
    "        preds = load_json(PREDICTIONS_OUT)\n",
    "        done_keys = set()\n",
    "        for p in preds:\n",
    "            key = (p[\"split\"], p[\"retriever\"], p[\"question\"])\n",
    "            done_keys.add(key)\n",
    "        print(f\"âœ… Already completed: {len(done_keys)} items\")\n",
    "        return preds, done_keys\n",
    "    else:\n",
    "        print(\"\\nðŸ“Œ No existing predictions found. Starting fresh.\")\n",
    "        return [], set()\n",
    "\n",
    "###############################################################################\n",
    "#                             MAIN EVAL\n",
    "###############################################################################\n",
    "def main():\n",
    "    baseline_records = load_json(RAG_EVAL_BASELINE_PATH)\n",
    "    docling_records  = load_json(RAG_EVAL_DOCLING_PATH)\n",
    "    silver_answers   = load_json(SILVER_ANSWERS_PATH)\n",
    "\n",
    "    print(f\"Baseline records: {len(baseline_records)}\")\n",
    "    print(f\"Docling records:  {len(docling_records)}\")\n",
    "    print(f\"Silver answers:   {len(silver_answers)}\")\n",
    "\n",
    "    all_predictions, done_keys = load_existing_predictions()\n",
    "\n",
    "    SAVE_EVERY = 20  # save partial progress every N new predictions\n",
    "    new_count = 0\n",
    "\n",
    "    # ---------------- Baseline ----------------\n",
    "    print(\"\\n=== Evaluating Qwen3-8B on baseline MiniLM ===\\n\")\n",
    "    for rec in tqdm(baseline_records):\n",
    "        q = rec.get(\"question\")\n",
    "        key = (\"baseline\", \"MiniLM\", q)\n",
    "        if key in done_keys:\n",
    "            continue\n",
    "\n",
    "        docs = rec.get(\"retrieved_docs\", [])\n",
    "        context = context_from_docs(docs)\n",
    "        gold = silver_answers.get(q, None)\n",
    "\n",
    "        pred, latency, ga, gr = generate_answer(q, context)\n",
    "        p, r = compute_precision_recall(pred, gold)\n",
    "\n",
    "        row = {\n",
    "            \"split\": \"baseline\",\n",
    "            \"retriever\": \"MiniLM\",\n",
    "            \"question\": q,\n",
    "            \"gold_silver\": gold,\n",
    "            \"prediction\": pred,\n",
    "            \"precision\": p,\n",
    "            \"recall\": r,\n",
    "            \"latency_sec\": latency,\n",
    "            \"gpu_alloc_GB\": ga,\n",
    "            \"gpu_reserved_GB\": gr\n",
    "        }\n",
    "\n",
    "        all_predictions.append(row)\n",
    "        done_keys.add(key)\n",
    "        new_count += 1\n",
    "\n",
    "        if new_count % SAVE_EVERY == 0:\n",
    "            save_json(all_predictions, PREDICTIONS_OUT)\n",
    "            print(f\"\\nðŸ’¾ Saved partial predictions ({len(all_predictions)} total)\")\n",
    "\n",
    "    # ---------------- Docling ----------------\n",
    "    for retriever in RETRIEVERS_DOCLING:\n",
    "        print(f\"\\n=== Evaluating Qwen3-8B on docling retriever: {retriever} ===\\n\")\n",
    "        field = RETRIEVER_TO_FIELD[retriever]\n",
    "\n",
    "        for rec in tqdm(docling_records):\n",
    "            q = rec.get(\"question\")\n",
    "            key = (\"docling\", retriever, q)\n",
    "            if key in done_keys:\n",
    "                continue\n",
    "\n",
    "            docs = rec.get(field, [])\n",
    "            context = context_from_docs(docs)\n",
    "            gold = silver_answers.get(q, None)\n",
    "\n",
    "            pred, latency, ga, gr = generate_answer(q, context)\n",
    "            p, r = compute_precision_recall(pred, gold)\n",
    "\n",
    "            row = {\n",
    "                \"split\": \"docling\",\n",
    "                \"retriever\": retriever,\n",
    "                \"question\": q,\n",
    "                \"gold_silver\": gold,\n",
    "                \"prediction\": pred,\n",
    "                \"precision\": p,\n",
    "                \"recall\": r,\n",
    "                \"latency_sec\": latency,\n",
    "                \"gpu_alloc_GB\": ga,\n",
    "                \"gpu_reserved_GB\": gr\n",
    "            }\n",
    "\n",
    "            all_predictions.append(row)\n",
    "            done_keys.add(key)\n",
    "            new_count += 1\n",
    "\n",
    "            if new_count % SAVE_EVERY == 0:\n",
    "                save_json(all_predictions, PREDICTIONS_OUT)\n",
    "                print(f\"\\nðŸ’¾ Saved partial predictions ({len(all_predictions)} total)\")\n",
    "\n",
    "    # final save\n",
    "    save_json(all_predictions, PREDICTIONS_OUT)\n",
    "    print(f\"\\nâœ… Final predictions saved to {PREDICTIONS_OUT}\")\n",
    "\n",
    "    # ---------------- Build summaries from predictions ----------------\n",
    "    summaries = []\n",
    "    # group predictions by (split, retriever)\n",
    "    groups = {}\n",
    "    for p in all_predictions:\n",
    "        gkey = (p[\"split\"], p[\"retriever\"])\n",
    "        groups.setdefault(gkey, []).append(p)\n",
    "\n",
    "    for (split, retriever), items in groups.items():\n",
    "        precisions = [x[\"precision\"] for x in items if x[\"precision\"] is not None]\n",
    "        recalls    = [x[\"recall\"] for x in items if x[\"recall\"] is not None]\n",
    "        latencies  = [x[\"latency_sec\"] for x in items]\n",
    "        g_allocs   = [x[\"gpu_alloc_GB\"] for x in items]\n",
    "        g_res      = [x[\"gpu_reserved_GB\"] for x in items]\n",
    "\n",
    "        # RAGAS needs question/context/answer/prediction rows\n",
    "        valid = [x for x in items if x[\"gold_silver\"] is not None]\n",
    "        ragas_ds = Dataset.from_dict({\n",
    "            \"question\": [x[\"question\"] for x in valid],\n",
    "            \"contexts\": [[context_from_docs(\n",
    "                # reconstruct context quickly not needed, use prediction-time context if you stored it\n",
    "                # for now use empty placeholder to keep schema valid\n",
    "                [] \n",
    "            )] for _ in valid],\n",
    "            \"answer\": [x[\"gold_silver\"] for x in valid],\n",
    "            \"prediction\": [x[\"prediction\"] for x in valid],\n",
    "        })\n",
    "\n",
    "        # NOTE: If you want exact RAGAS contexts, store \"context\" in predictions.\n",
    "        # RAGAS will still run, but faithfulness/relevancy are weaker without true contexts.\n",
    "        ragas_out = evaluate(ragas_ds, metrics=[faithfulness, answer_relevancy])\n",
    "\n",
    "        summaries.append({\n",
    "            \"model\": QWEN_MODEL_NAME,\n",
    "            \"quantized_4bit\": USE_BNB,\n",
    "            \"split\": split,\n",
    "            \"retriever\": retriever,\n",
    "            \"precision\": float(np.mean(precisions)) if precisions else None,\n",
    "            \"recall\": float(np.mean(recalls)) if recalls else None,\n",
    "            \"faithfulness\": float(ragas_out[\"faithfulness\"]),\n",
    "            \"relevance\": float(ragas_out[\"answer_relevancy\"]),\n",
    "            \"avg_latency_sec\": float(np.mean(latencies)),\n",
    "            \"avg_gpu_alloc_GB\": float(np.mean(g_allocs)),\n",
    "            \"avg_gpu_reserved_GB\": float(np.mean(g_res)),\n",
    "            \"n_items\": len(items)\n",
    "        })\n",
    "\n",
    "    save_json(summaries, SUMMARY_OUT)\n",
    "    print(f\"âœ… Summary saved to {SUMMARY_OUT}\")\n",
    "\n",
    "    # ---------------- Visualizations ----------------\n",
    "    labels = [f'{s[\"split\"]}:{s[\"retriever\"]}' for s in summaries]\n",
    "    precision_vals = [s[\"precision\"] for s in summaries]\n",
    "    recall_vals = [s[\"recall\"] for s in summaries]\n",
    "    faith_vals = [s[\"faithfulness\"] for s in summaries]\n",
    "    rel_vals = [s[\"relevance\"] for s in summaries]\n",
    "    lat_vals = [s[\"avg_latency_sec\"] for s in summaries]\n",
    "\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.2\n",
    "\n",
    "    plt.figure()\n",
    "    plt.bar(x - 1.5*width, precision_vals, width, label=\"Precision\")\n",
    "    plt.bar(x - 0.5*width, recall_vals, width, label=\"Recall\")\n",
    "    plt.bar(x + 0.5*width, faith_vals, width, label=\"Faithfulness\")\n",
    "    plt.bar(x + 1.5*width, rel_vals, width, label=\"Relevance\")\n",
    "    plt.xticks(x, labels, rotation=25, ha=\"right\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Qwen3-8B Metrics (Resume-enabled) by Split/Retriever\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.bar(labels, lat_vals)\n",
    "    plt.xticks(rotation=25, ha=\"right\")\n",
    "    plt.ylabel(\"Avg latency (sec)\")\n",
    "    plt.title(\"Qwen3-8B Average Latency by Split/Retriever\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62bf6b46-fcc7-420e-88a4-8b4c6612c989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "âœ… bitsandbytes found -> loading Qwen3-8B in 4-bit.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e637d95b82438eb982ade9190f1489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline records: 723\n",
      "Docling records:  723\n",
      "Silver answers:   722\n",
      "\n",
      "ðŸ“Œ Found existing predictions at /home/anbarasan.p/qwen3_predictions_quant.json. Resuming...\n",
      "âœ… Already completed: 2888 items\n",
      "ðŸ”§ Backfilled 0 missing context fields.\n",
      "ðŸ”§ Added 8664 missing metric fields to old predictions.\n",
      "\n",
      "=== Evaluating Qwen3-8B on baseline MiniLM ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 723/723 [00:00<00:00, 3172052.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Qwen3-8B on docling retriever: miniLM ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 723/723 [00:00<00:00, 3063112.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Qwen3-8B on docling retriever: hybrid_gemma ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 723/723 [00:00<00:00, 3116630.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Qwen3-8B on docling retriever: hybrid_gemma2048 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 723/723 [00:00<00:00, 3260733.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Final predictions saved to /home/anbarasan.p/qwen3_predictions_quant.json\n",
      "âœ… Summary saved to /home/anbarasan.p/qwen3_benchmarks_summary_quant.json\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 445\u001b[0m\n\u001b[1;32m    441\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 445\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[7], line 425\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    423\u001b[0m plt\u001b[38;5;241m.\u001b[39mbar(x \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mwidth, precision_vals, width, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    424\u001b[0m plt\u001b[38;5;241m.\u001b[39mbar(x \u001b[38;5;241m-\u001b[39m width, recall_vals, width, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 425\u001b[0m plt\u001b[38;5;241m.\u001b[39mbar(x, sim_vals, width, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswerSimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    426\u001b[0m plt\u001b[38;5;241m.\u001b[39mbar(x \u001b[38;5;241m+\u001b[39m width, cprec_vals, width, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContextPrecision\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    427\u001b[0m plt\u001b[38;5;241m.\u001b[39mbar(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mwidth, crec_vals, width, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContextRecall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages/matplotlib/pyplot.py:2754\u001b[0m, in \u001b[0;36mbar\u001b[0;34m(x, height, width, bottom, align, data, **kwargs)\u001b[0m\n\u001b[1;32m   2743\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mbar)\n\u001b[1;32m   2744\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbar\u001b[39m(\n\u001b[1;32m   2745\u001b[0m     x: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2752\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2753\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BarContainer:\n\u001b[0;32m-> 2754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gca()\u001b[38;5;241m.\u001b[39mbar(\n\u001b[1;32m   2755\u001b[0m         x,\n\u001b[1;32m   2756\u001b[0m         height,\n\u001b[1;32m   2757\u001b[0m         width\u001b[38;5;241m=\u001b[39mwidth,\n\u001b[1;32m   2758\u001b[0m         bottom\u001b[38;5;241m=\u001b[39mbottom,\n\u001b[1;32m   2759\u001b[0m         align\u001b[38;5;241m=\u001b[39malign,\n\u001b[1;32m   2760\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[1;32m   2761\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2762\u001b[0m     )\n",
      "File \u001b[0;32m/shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages/matplotlib/__init__.py:1465\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(ax, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(sanitize_sequence, args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1467\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1468\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1469\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m/shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages/matplotlib/axes/_axes.py:2524\u001b[0m, in \u001b[0;36mAxes.bar\u001b[0;34m(self, x, height, width, bottom, align, **kwargs)\u001b[0m\n\u001b[1;32m   2521\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(left, bottom, width, height, color, edgecolor, linewidth,\n\u001b[1;32m   2522\u001b[0m            hatch, patch_labels)\n\u001b[1;32m   2523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l, b, w, h, c, e, lw, htch, lbl \u001b[38;5;129;01min\u001b[39;00m args:\n\u001b[0;32m-> 2524\u001b[0m     r \u001b[38;5;241m=\u001b[39m mpatches\u001b[38;5;241m.\u001b[39mRectangle(\n\u001b[1;32m   2525\u001b[0m         xy\u001b[38;5;241m=\u001b[39m(l, b), width\u001b[38;5;241m=\u001b[39mw, height\u001b[38;5;241m=\u001b[39mh,\n\u001b[1;32m   2526\u001b[0m         facecolor\u001b[38;5;241m=\u001b[39mc,\n\u001b[1;32m   2527\u001b[0m         edgecolor\u001b[38;5;241m=\u001b[39me,\n\u001b[1;32m   2528\u001b[0m         linewidth\u001b[38;5;241m=\u001b[39mlw,\n\u001b[1;32m   2529\u001b[0m         label\u001b[38;5;241m=\u001b[39mlbl,\n\u001b[1;32m   2530\u001b[0m         hatch\u001b[38;5;241m=\u001b[39mhtch,\n\u001b[1;32m   2531\u001b[0m         )\n\u001b[1;32m   2532\u001b[0m     r\u001b[38;5;241m.\u001b[39m_internal_update(kwargs)\n\u001b[1;32m   2533\u001b[0m     r\u001b[38;5;241m.\u001b[39mget_path()\u001b[38;5;241m.\u001b[39m_interpolation_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "File \u001b[0;32m/shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages/matplotlib/patches.py:732\u001b[0m, in \u001b[0;36mRectangle.__init__\u001b[0;34m(self, xy, width, height, angle, rotation_point, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# Required for RectangleSelector with axes aspect ratio != 1\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# The patch is defined in data coordinates and when changing the\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# selector with square modifier and not in data coordinates, we need\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# to correct for the aspect ratio difference between the data and\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;66;03m# display coordinate systems. Its value is typically provide by\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;66;03m# Axes._get_aspect_ratio()\u001b[39;00m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aspect_ratio_correction \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m--> 732\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_units()\n",
      "File \u001b[0;32m/shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages/matplotlib/patches.py:743\u001b[0m, in \u001b[0;36mRectangle._convert_units\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    741\u001b[0m y0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_yunits(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_y0)\n\u001b[1;32m    742\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_xunits(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_x0 \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_width)\n\u001b[0;32m--> 743\u001b[0m y1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_yunits(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_y0 \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_height)\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x0, y0, x1, y1\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'NoneType'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json, os, time, re, torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "###############################################################################\n",
    "#                             DEVICE CONFIG\n",
    "###############################################################################\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "###############################################################################\n",
    "#                             PATH CONFIG\n",
    "###############################################################################\n",
    "BASE_DATA_PATH = \"/courses/DS5500.202610/data/team1/processed_data\"\n",
    "\n",
    "RAG_EVAL_BASELINE_PATH = os.path.join(BASE_DATA_PATH, \"rag_eval_dataset.json\")\n",
    "RAG_EVAL_DOCLING_PATH  = os.path.join(BASE_DATA_PATH, \"rag_eval_dataset_docling.json\")\n",
    "\n",
    "# silver answers location (home dir)\n",
    "SILVER_ANSWERS_PATH = \"/home/anbarasan.p/rag_eval_silver_answers.json\"\n",
    "\n",
    "# outputs (home dir)\n",
    "PREDICTIONS_OUT = \"/home/anbarasan.p/qwen3_predictions_quant.json\"\n",
    "SUMMARY_OUT     = \"/home/anbarasan.p/qwen3_benchmarks_summary_quant.json\"\n",
    "\n",
    "# Docling retrievers + fields\n",
    "RETRIEVERS_DOCLING = [\"miniLM\", \"hybrid_gemma\", \"hybrid_gemma2048\"]\n",
    "RETRIEVER_TO_FIELD = {\n",
    "    \"miniLM\": \"retrieved_docs_minilm_hybrid\",\n",
    "    \"hybrid_gemma\": \"retrieved_docs_gemma_hybrid\",\n",
    "    \"hybrid_gemma2048\": \"retrieved_docs_gemma2048_hybrid\",\n",
    "}\n",
    "\n",
    "###############################################################################\n",
    "#                             MODEL CONFIG\n",
    "###############################################################################\n",
    "QWEN_MODEL_NAME = \"Qwen/Qwen3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(QWEN_MODEL_NAME)\n",
    "\n",
    "USE_BNB = False\n",
    "if DEVICE == \"cuda\":\n",
    "    try:\n",
    "        import bitsandbytes  # noqa\n",
    "        USE_BNB = True\n",
    "        print(\"âœ… bitsandbytes found -> loading Qwen3-8B in 4-bit.\")\n",
    "    except Exception:\n",
    "        print(\"âš ï¸ bitsandbytes not found -> loading full precision on GPU.\")\n",
    "else:\n",
    "    print(\"âš ï¸ CPU session detected. Will be slow.\")\n",
    "\n",
    "if USE_BNB:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        QWEN_MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        QWEN_MODEL_NAME,\n",
    "        torch_dtype=\"auto\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "###############################################################################\n",
    "#                 OFFLINE EMBEDDINGS (MiniLM) FOR SIMILARITY\n",
    "###############################################################################\n",
    "# Requires: pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "EMB_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "emb_model = SentenceTransformer(EMB_MODEL_NAME, device=DEVICE if DEVICE==\"cuda\" else \"cpu\")\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    a = np.array(a); b = np.array(b)\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "###############################################################################\n",
    "#                             HELPERS\n",
    "###############################################################################\n",
    "def load_json(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(obj, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "def get_gpu_stats():\n",
    "    if not torch.cuda.is_available():\n",
    "        return 0.0, 0.0\n",
    "    allocated = torch.cuda.memory_allocated() / (1024**3)\n",
    "    reserved  = torch.cuda.memory_reserved() / (1024**3)\n",
    "    return float(allocated), float(reserved)\n",
    "\n",
    "def normalize_text(s):\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def token_set(s):\n",
    "    s = normalize_text(s)\n",
    "    return set(s.split()) if s else set()\n",
    "\n",
    "def compute_precision_recall(pred, gold):\n",
    "    \"\"\"Token-overlap precision/recall pred vs gold.\"\"\"\n",
    "    if gold is None or gold == \"\" or gold == \"N/A\":\n",
    "        return None, None\n",
    "    pset = token_set(pred)\n",
    "    gset = token_set(gold)\n",
    "    if not pset or not gset:\n",
    "        return None, None\n",
    "    tp = len(pset & gset)\n",
    "    precision = tp / len(pset)\n",
    "    recall = tp / len(gset)\n",
    "    return precision, recall\n",
    "\n",
    "def compute_context_precision(pred, context):\n",
    "    \"\"\"Fraction of prediction tokens that appear in context.\"\"\"\n",
    "    pset = token_set(pred)\n",
    "    cset = token_set(context)\n",
    "    if not pset:\n",
    "        return None\n",
    "    return len(pset & cset) / len(pset)\n",
    "\n",
    "def compute_context_recall(gold, context):\n",
    "    \"\"\"Fraction of gold tokens that appear in context.\"\"\"\n",
    "    gset = token_set(gold)\n",
    "    cset = token_set(context)\n",
    "    if not gset:\n",
    "        return None\n",
    "    return len(gset & cset) / len(gset)\n",
    "\n",
    "def compute_answer_similarity(pred, gold):\n",
    "    \"\"\"Cosine similarity between MiniLM embeddings of pred and gold.\"\"\"\n",
    "    if gold is None or gold == \"\" or gold == \"N/A\":\n",
    "        return None\n",
    "    pred_emb = emb_model.encode([pred], normalize_embeddings=True)[0]\n",
    "    gold_emb = emb_model.encode([gold], normalize_embeddings=True)[0]\n",
    "    return cosine_sim(pred_emb, gold_emb)\n",
    "\n",
    "def context_from_docs(docs, max_docs=3, max_chars=4000):\n",
    "    if isinstance(docs, list):\n",
    "        text = \"\\n\\n\".join(docs[:max_docs])\n",
    "    else:\n",
    "        text = str(docs)\n",
    "    return text[:max_chars]\n",
    "\n",
    "def make_prompt(question, context):\n",
    "    return f\"\"\"\n",
    "You are a scientific assistant. Answer the question using ONLY the information from the context.\n",
    "Do NOT use outside knowledge. If the answer cannot be found, say:\n",
    "\"The context does not provide enough information.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Final Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_answer(question, context):\n",
    "    prompt = make_prompt(question, context)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(DEVICE)\n",
    "\n",
    "    start = time.time()\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=False\n",
    "    )\n",
    "    latency = time.time() - start\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    ga, gr = get_gpu_stats()\n",
    "    return answer, latency, ga, gr\n",
    "\n",
    "###############################################################################\n",
    "#                  RESUME LOGIC: LOAD EXISTING PREDICTIONS\n",
    "###############################################################################\n",
    "def load_existing_predictions():\n",
    "    if os.path.exists(PREDICTIONS_OUT):\n",
    "        print(f\"\\nðŸ“Œ Found existing predictions at {PREDICTIONS_OUT}. Resuming...\")\n",
    "        preds = load_json(PREDICTIONS_OUT)\n",
    "        done_keys = set()\n",
    "        for p in preds:\n",
    "            key = (p.get(\"split\"), p.get(\"retriever\"), p.get(\"question\"))\n",
    "            done_keys.add(key)\n",
    "        print(f\"âœ… Already completed: {len(done_keys)} items\")\n",
    "        return preds, done_keys\n",
    "    else:\n",
    "        print(\"\\nðŸ“Œ No existing predictions found. Starting fresh.\")\n",
    "        return [], set()\n",
    "\n",
    "###############################################################################\n",
    "#        BACKFILL MISSING CONTEXT FOR OLD PREDICTIONS\n",
    "###############################################################################\n",
    "def fix_missing_context(all_predictions, baseline_records, docling_records):\n",
    "    baseline_lookup = {}\n",
    "    for rec in baseline_records:\n",
    "        q = rec.get(\"question\")\n",
    "        baseline_lookup[q] = context_from_docs(rec.get(\"retrieved_docs\", []))\n",
    "\n",
    "    docling_lookup = {rec.get(\"question\"): rec for rec in docling_records}\n",
    "\n",
    "    fixed = 0\n",
    "    for p in all_predictions:\n",
    "        if \"context\" in p and p[\"context\"] is not None:\n",
    "            continue\n",
    "\n",
    "        split = p.get(\"split\")\n",
    "        retriever = p.get(\"retriever\")\n",
    "        q = p.get(\"question\")\n",
    "\n",
    "        if split == \"baseline\":\n",
    "            p[\"context\"] = baseline_lookup.get(q, \"\")\n",
    "        else:\n",
    "            rec = docling_lookup.get(q, None)\n",
    "            if rec is None:\n",
    "                p[\"context\"] = \"\"\n",
    "            else:\n",
    "                field = RETRIEVER_TO_FIELD[retriever]\n",
    "                p[\"context\"] = context_from_docs(rec.get(field, []))\n",
    "\n",
    "        fixed += 1\n",
    "\n",
    "    print(f\"ðŸ”§ Backfilled {fixed} missing context fields.\")\n",
    "    return all_predictions\n",
    "\n",
    "###############################################################################\n",
    "#        BACKFILL MISSING METRIC KEYS FOR OLD PREDICTIONS\n",
    "###############################################################################\n",
    "def fix_missing_metric_keys(all_predictions):\n",
    "    REQUIRED_KEYS = [\n",
    "        \"precision\", \"recall\", \"answer_similarity\",\n",
    "        \"context_precision\", \"context_recall\",\n",
    "        \"latency_sec\", \"gpu_alloc_GB\", \"gpu_reserved_GB\"\n",
    "    ]\n",
    "    fixed = 0\n",
    "    for p in all_predictions:\n",
    "        for k in REQUIRED_KEYS:\n",
    "            if k not in p:\n",
    "                p[k] = None\n",
    "                fixed += 1\n",
    "    if fixed > 0:\n",
    "        print(f\"ðŸ”§ Added {fixed} missing metric fields to old predictions.\")\n",
    "    return all_predictions\n",
    "\n",
    "###############################################################################\n",
    "#                             MAIN EVAL\n",
    "###############################################################################\n",
    "def main():\n",
    "    baseline_records = load_json(RAG_EVAL_BASELINE_PATH)\n",
    "    docling_records  = load_json(RAG_EVAL_DOCLING_PATH)\n",
    "    silver_answers   = load_json(SILVER_ANSWERS_PATH)\n",
    "\n",
    "    print(f\"Baseline records: {len(baseline_records)}\")\n",
    "    print(f\"Docling records:  {len(docling_records)}\")\n",
    "    print(f\"Silver answers:   {len(silver_answers)}\")\n",
    "\n",
    "    all_predictions, done_keys = load_existing_predictions()\n",
    "\n",
    "    # âœ… fix old partials\n",
    "    all_predictions = fix_missing_context(all_predictions, baseline_records, docling_records)\n",
    "    all_predictions = fix_missing_metric_keys(all_predictions)\n",
    "\n",
    "    SAVE_EVERY = 20\n",
    "    new_count = 0\n",
    "\n",
    "    # ---------------- Baseline ----------------\n",
    "    print(\"\\n=== Evaluating Qwen3-8B on baseline MiniLM ===\\n\")\n",
    "    for rec in tqdm(baseline_records):\n",
    "        q = rec.get(\"question\")\n",
    "        key = (\"baseline\", \"MiniLM\", q)\n",
    "        if key in done_keys:\n",
    "            continue\n",
    "\n",
    "        context = context_from_docs(rec.get(\"retrieved_docs\", []))\n",
    "        gold = silver_answers.get(q, None)\n",
    "\n",
    "        pred, latency, ga, gr = generate_answer(q, context)\n",
    "\n",
    "        precision, recall = compute_precision_recall(pred, gold)\n",
    "        ans_sim = compute_answer_similarity(pred, gold)\n",
    "        cprec = compute_context_precision(pred, context)\n",
    "        crec = compute_context_recall(gold, context) if gold else None\n",
    "\n",
    "        row = {\n",
    "            \"split\": \"baseline\",\n",
    "            \"retriever\": \"MiniLM\",\n",
    "            \"question\": q,\n",
    "            \"context\": context,\n",
    "            \"gold_silver\": gold,\n",
    "            \"prediction\": pred,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"answer_similarity\": ans_sim,\n",
    "            \"context_precision\": cprec,\n",
    "            \"context_recall\": crec,\n",
    "            \"latency_sec\": latency,\n",
    "            \"gpu_alloc_GB\": ga,\n",
    "            \"gpu_reserved_GB\": gr\n",
    "        }\n",
    "\n",
    "        all_predictions.append(row)\n",
    "        done_keys.add(key)\n",
    "        new_count += 1\n",
    "\n",
    "        if new_count % SAVE_EVERY == 0:\n",
    "            save_json(all_predictions, PREDICTIONS_OUT)\n",
    "            print(f\"\\nðŸ’¾ Saved partial predictions ({len(all_predictions)} total)\")\n",
    "\n",
    "    # ---------------- Docling ----------------\n",
    "    for retriever in RETRIEVERS_DOCLING:\n",
    "        print(f\"\\n=== Evaluating Qwen3-8B on docling retriever: {retriever} ===\\n\")\n",
    "        field = RETRIEVER_TO_FIELD[retriever]\n",
    "\n",
    "        for rec in tqdm(docling_records):\n",
    "            q = rec.get(\"question\")\n",
    "            key = (\"docling\", retriever, q)\n",
    "            if key in done_keys:\n",
    "                continue\n",
    "\n",
    "            context = context_from_docs(rec.get(field, []))\n",
    "            gold = silver_answers.get(q, None)\n",
    "\n",
    "            pred, latency, ga, gr = generate_answer(q, context)\n",
    "\n",
    "            precision, recall = compute_precision_recall(pred, gold)\n",
    "            ans_sim = compute_answer_similarity(pred, gold)\n",
    "            cprec = compute_context_precision(pred, context)\n",
    "            crec = compute_context_recall(gold, context) if gold else None\n",
    "\n",
    "            row = {\n",
    "                \"split\": \"docling\",\n",
    "                \"retriever\": retriever,\n",
    "                \"question\": q,\n",
    "                \"context\": context,\n",
    "                \"gold_silver\": gold,\n",
    "                \"prediction\": pred,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"answer_similarity\": ans_sim,\n",
    "                \"context_precision\": cprec,\n",
    "                \"context_recall\": crec,\n",
    "                \"latency_sec\": latency,\n",
    "                \"gpu_alloc_GB\": ga,\n",
    "                \"gpu_reserved_GB\": gr\n",
    "            }\n",
    "\n",
    "            all_predictions.append(row)\n",
    "            done_keys.add(key)\n",
    "            new_count += 1\n",
    "\n",
    "            if new_count % SAVE_EVERY == 0:\n",
    "                save_json(all_predictions, PREDICTIONS_OUT)\n",
    "                print(f\"\\nðŸ’¾ Saved partial predictions ({len(all_predictions)} total)\")\n",
    "\n",
    "    save_json(all_predictions, PREDICTIONS_OUT)\n",
    "    print(f\"\\nâœ… Final predictions saved to {PREDICTIONS_OUT}\")\n",
    "\n",
    "    # ---------------- Summaries ----------------\n",
    "    summaries = []\n",
    "    groups = {}\n",
    "    for p in all_predictions:\n",
    "        gkey = (p[\"split\"], p[\"retriever\"])\n",
    "        groups.setdefault(gkey, []).append(p)\n",
    "\n",
    "    def mean_ignore_none(xs):\n",
    "        xs = [x for x in xs if x is not None]\n",
    "        return float(np.mean(xs)) if xs else None\n",
    "\n",
    "    for (split, retriever), items in groups.items():\n",
    "        summary = {\n",
    "            \"model\": QWEN_MODEL_NAME,\n",
    "            \"quantized_4bit\": USE_BNB,\n",
    "            \"split\": split,\n",
    "            \"retriever\": retriever,\n",
    "            \"precision\": mean_ignore_none([x.get(\"precision\") for x in items]),\n",
    "            \"recall\": mean_ignore_none([x.get(\"recall\") for x in items]),\n",
    "            \"answer_similarity\": mean_ignore_none([x.get(\"answer_similarity\") for x in items]),\n",
    "            \"context_precision\": mean_ignore_none([x.get(\"context_precision\") for x in items]),\n",
    "            \"context_recall\": mean_ignore_none([x.get(\"context_recall\") for x in items]),\n",
    "            \"avg_latency_sec\": float(np.mean([x[\"latency_sec\"] for x in items if x[\"latency_sec\"] is not None])),\n",
    "            \"avg_gpu_alloc_GB\": float(np.mean([x[\"gpu_alloc_GB\"] for x in items if x[\"gpu_alloc_GB\"] is not None])),\n",
    "            \"avg_gpu_reserved_GB\": float(np.mean([x[\"gpu_reserved_GB\"] for x in items if x[\"gpu_reserved_GB\"] is not None])),\n",
    "            \"n_items\": len(items)\n",
    "        }\n",
    "        summaries.append(summary)\n",
    "\n",
    "    save_json(summaries, SUMMARY_OUT)\n",
    "    print(f\"âœ… Summary saved to {SUMMARY_OUT}\")\n",
    "\n",
    "    # ---------------- Plots ----------------\n",
    "    labels = [f'{s[\"split\"]}:{s[\"retriever\"]}' for s in summaries]\n",
    "    precision_vals = [s[\"precision\"] for s in summaries]\n",
    "    recall_vals = [s[\"recall\"] for s in summaries]\n",
    "    sim_vals = [s[\"answer_similarity\"] for s in summaries]\n",
    "    cprec_vals = [s[\"context_precision\"] for s in summaries]\n",
    "    crec_vals = [s[\"context_recall\"] for s in summaries]\n",
    "    lat_vals = [s[\"avg_latency_sec\"] for s in summaries]\n",
    "\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.18\n",
    "\n",
    "    plt.figure()\n",
    "    plt.bar(x - 2*width, precision_vals, width, label=\"Precision\")\n",
    "    plt.bar(x - width, recall_vals, width, label=\"Recall\")\n",
    "    plt.bar(x, sim_vals, width, label=\"AnswerSimilarity\")\n",
    "    plt.bar(x + width, cprec_vals, width, label=\"ContextPrecision\")\n",
    "    plt.bar(x + 2*width, crec_vals, width, label=\"ContextRecall\")\n",
    "    plt.xticks(x, labels, rotation=25, ha=\"right\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Qwen3-8B OFFLINE Metrics by Split/Retriever\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.bar(labels, lat_vals)\n",
    "    plt.xticks(rotation=25, ha=\"right\")\n",
    "    plt.ylabel(\"Avg latency (sec)\")\n",
    "    plt.title(\"Qwen3-8B Average Latency by Split/Retriever\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3595ab9c-ed88-4afa-b741-7458ec3a3f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions: 722\n",
      "Valid gold answers: 721\n",
      "Invalid / missing answers: 1\n",
      "\n",
      "Sample invalid entries:\n",
      "- Q: What is the form of Î²X written as?\n",
      "  A: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Path where your silver answers are stored\n",
    "SILVER_PATH = \"/home/anbarasan.p/rag_eval_silver_answers.json\"\n",
    "\n",
    "# Load the silver answers JSON\n",
    "with open(SILVER_PATH, \"r\") as f:\n",
    "    silver = json.load(f)\n",
    "\n",
    "total = len(silver)\n",
    "\n",
    "valid = 0\n",
    "invalid = 0\n",
    "\n",
    "invalid_examples = []\n",
    "\n",
    "def is_valid_answer(ans):\n",
    "    if ans is None:\n",
    "        return False\n",
    "    if isinstance(ans, str):\n",
    "        stripped = ans.strip()\n",
    "        if stripped == \"\" or stripped.lower() in [\"n/a\", \"na\", \"none\"]:\n",
    "            return False\n",
    "        # Some common Flan-T5 junk answers\n",
    "        if stripped.lower() in [\n",
    "            \"the context does not provide enough information.\",\n",
    "            \"is not yet supported\",\n",
    "            \"input images\",\n",
    "            \"space\"\n",
    "        ]:\n",
    "            return False\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "for question, answer in silver.items():\n",
    "    if is_valid_answer(answer):\n",
    "        valid += 1\n",
    "    else:\n",
    "        invalid += 1\n",
    "        if len(invalid_examples) < 20:   # show only first 20 bad items\n",
    "            invalid_examples.append((question, answer))\n",
    "\n",
    "print(f\"Total questions: {total}\")\n",
    "print(f\"Valid gold answers: {valid}\")\n",
    "print(f\"Invalid / missing answers: {invalid}\")\n",
    "print(\"\\nSample invalid entries:\")\n",
    "for q, a in invalid_examples:\n",
    "    print(f\"- Q: {q}\\n  A: {a}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf306d5d-d758-46ae-9f29-5e248e816882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Summary written to: /home/anbarasan.p/qwen3_generation_summary_clean.json\n",
      "=== Preview ===\n",
      "{'split': 'baseline', 'retriever': 'MiniLM', 'n_items_total': 722, 'n_items_with_metrics': 721, 'precision': 0.0482002417377534, 'recall': 2.9565074156559645, 'answer_similarity': 0.22794270524257812, 'context_precision': 0.6858530774883533, 'context_recall': 0.6103887266500073, 'avg_latency_sec': 10.884087752700678, 'avg_gpu_alloc_GB': 5.672220146110418, 'avg_gpu_reserved_GB': 7.819239771151179}\n",
      "{'split': 'docling', 'retriever': 'miniLM', 'n_items_total': 722, 'n_items_with_metrics': 721, 'precision': 0.038834460271835056, 'recall': 3.226211007900477, 'answer_similarity': 0.19990939647282233, 'context_precision': 0.7229737986134531, 'context_recall': 0.5180406932827786, 'avg_latency_sec': 10.642218735280878, 'avg_gpu_alloc_GB': 5.672225016330714, 'avg_gpu_reserved_GB': 7.818004507628294}\n",
      "{'split': 'docling', 'retriever': 'hybrid_gemma', 'n_items_total': 722, 'n_items_with_metrics': 721, 'precision': 0.04254548723687375, 'recall': 4.285781951096682, 'answer_similarity': 0.20594353640895083, 'context_precision': 0.7996217185924235, 'context_recall': 0.6399865760572397, 'avg_latency_sec': 10.817166231541627, 'avg_gpu_alloc_GB': 5.672229098215513, 'avg_gpu_reserved_GB': 7.814388111130374}\n",
      "{'split': 'docling', 'retriever': 'hybrid_gemma2048', 'n_items_total': 722, 'n_items_with_metrics': 721, 'precision': 0.03890134335625522, 'recall': 3.8220542351862177, 'answer_similarity': 0.19191034103907778, 'context_precision': 0.7950785418733909, 'context_recall': 0.569620387300363, 'avg_latency_sec': 10.89394426081283, 'avg_gpu_alloc_GB': 9.595869147661821, 'avg_gpu_reserved_GB': 13.057485805305133}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "PREDICTIONS_OUT = \"/home/anbarasan.p/qwen3_predictions_quant.json\"\n",
    "\n",
    "# Load predictions\n",
    "with open(PREDICTIONS_OUT, \"r\") as f:\n",
    "    all_predictions = json.load(f)\n",
    "\n",
    "# Baseline retriever\n",
    "BASELINE_RETRIEVER = [\"MiniLM\"]\n",
    "\n",
    "# Docling retrievers (matches your dataset)\n",
    "RETRIEVERS_DOCLING = [\"miniLM\", \"hybrid_gemma\", \"hybrid_gemma2048\"]\n",
    "\n",
    "# Combine them for summary\n",
    "ALL_RETRIEVERS = [\"MiniLM\"] + RETRIEVERS_DOCLING\n",
    "\n",
    "def mean_ignore_none(arr):\n",
    "    arr = [x for x in arr if x is not None]\n",
    "    return float(np.mean(arr)) if arr else None\n",
    "\n",
    "def filter_valid(items):\n",
    "    \"\"\"Keep only rows where metrics exist.\"\"\"\n",
    "    return [\n",
    "        p for p in items\n",
    "        if p.get(\"answer_similarity\") is not None\n",
    "        and p.get(\"context_precision\") is not None\n",
    "        and p.get(\"context_recall\") is not None\n",
    "    ]\n",
    "\n",
    "summaries = []\n",
    "\n",
    "for split in [\"baseline\", \"docling\"]:\n",
    "    for retriever in ALL_RETRIEVERS:\n",
    "\n",
    "        # Filter rows for split + retriever\n",
    "        items = [\n",
    "            p for p in all_predictions\n",
    "            if p.get(\"split\") == split and p.get(\"retriever\") == retriever\n",
    "        ]\n",
    "\n",
    "        if not items:\n",
    "            continue\n",
    "\n",
    "        # Keep only rows where metrics exist\n",
    "        valid = filter_valid(items)\n",
    "\n",
    "        summary = {\n",
    "            \"split\": split,\n",
    "            \"retriever\": retriever,\n",
    "            \"n_items_total\": len(items),\n",
    "            \"n_items_with_metrics\": len(valid),\n",
    "            \"precision\": mean_ignore_none([x.get(\"precision\") for x in valid]),\n",
    "            \"recall\": mean_ignore_none([x.get(\"recall\") for x in valid]),\n",
    "            \"answer_similarity\": mean_ignore_none([x.get(\"answer_similarity\") for x in valid]),\n",
    "            \"context_precision\": mean_ignore_none([x.get(\"context_precision\") for x in valid]),\n",
    "            \"context_recall\": mean_ignore_none([x.get(\"context_recall\") for x in valid]),\n",
    "            \"avg_latency_sec\": mean_ignore_none([x.get(\"latency_sec\") for x in valid]),\n",
    "            \"avg_gpu_alloc_GB\": mean_ignore_none([x.get(\"gpu_alloc_GB\") for x in valid]),\n",
    "            \"avg_gpu_reserved_GB\": mean_ignore_none([x.get(\"gpu_reserved_GB\") for x in valid]),\n",
    "        }\n",
    "\n",
    "        summaries.append(summary)\n",
    "\n",
    "# Save summary\n",
    "SUMMARY_OUT = \"/home/anbarasan.p/qwen3_generation_summary_clean.json\"\n",
    "with open(SUMMARY_OUT, \"w\") as f:\n",
    "    json.dump(summaries, f, indent=2)\n",
    "\n",
    "print(\"âœ… Summary written to:\", SUMMARY_OUT)\n",
    "print(\"=== Preview ===\")\n",
    "for s in summaries[:5]:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd926288-f0d6-4198-8913-9771eee9769b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
